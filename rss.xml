<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
     xmlns:atom="http://www.w3.org/2005/Atom">

  <channel>
    <title>Blue Lightning AI Daily</title>
    <link>https://yourdomain.com/podcast</link>
    <language>en-us</language>
    <copyright>© 2025 Blue Lightning</copyright>
    <lastBuildDate>Tue, 14 Oct 2025 01:40:48 GMT</lastBuildDate>
    <pubDate>Sun, 31 Aug 2025 12:00:00 GMT</pubDate>
     <itunes:category text="Business">
       <itunes:category text="Entrepreneurship"/>
     </itunes:category>
     <itunes:category text="Technology">
       <itunes:category text="Tech News"/>
     </itunes:category>
    <generator>RSS Builder</generator>
    <description>
      Blue Lightning AI Daily is your go-to AI podcast for creators, delivering fast, focused updates on the world of generative AI. We cover the latest breakthroughs in large language models (LLMs), AI video editing, AI photography, AI audio tools, and creative automation. Each episode gives digital creators, content marketers, and creative professionals clear insights into how AI is transforming storytelling, production, and the creator economy. Stay informed, stay creative, and stay ahead with daily AI news made simple.
    </description>

    <!-- iTunes / Apple Podcasts tags -->
    <itunes:author>Blue Lightning</itunes:author>
    <itunes:summary>
      Blue Lightning AI Daily is your go-to AI podcast for creators, delivering fast, focused updates on the world of generative AI. 
      We cover the latest breakthroughs in LLMs, AI video, AI audio, and creative tools.
    </itunes:summary>
    <itunes:explicit>no</itunes:explicit>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/blue-lighting-cover.jpg"/>
    <itunes:owner>
      <itunes:name>Blue Lightning</itunes:name>
      <itunes:email>ted@coey.com</itunes:email>
    </itunes:owner>

    <!-- Atom self-link (recommended for apps like Apple Podcasts / Overcast) -->
    <atom:link href="https://raw.githubusercontent.com/ted-coey/bluelightingpodcast/main/rss.xml" rel="self" type="application/rss+xml"/>

    <!-- Episodes will be inserted here as <item> blocks -->

  <item>
    <title>Nano Banana: The Meme-Powered Image Revolution</title>
    <description><![CDATA[Discover why creators are obsessed with “Nano Banana”—Google’s new high-consistency image tool in Gemini. Learn how stable likeness, fast multi-step edits, and watermark safeguards are changing workflows for artists, marketers, and everyday users. We break down costs, real-world uses, and pro tips for perfect prompts, plus why this fun meme-model is winning fans across the AI art community.]]></description>
    <pubDate>Sun, 31 Aug 2025 19:38:18 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/nano_banana_revolution.mp3" length="3761760" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756669098912</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+nano_banana_revolution.jpg"/>
  </item>

  <item>
    <title>Grok Code Fast 1: Blazing Agent Loops, Zero Lag</title>
    <description><![CDATA[We break down xAI’s Grok Code Fast 1, a lightning-speed coding model made for agents that build, test, and fix in sub-seconds. From creators to dev teams, learn how this low-latency engine transforms workflows, slashes context-switching, and enables affordable micro-automation. Find out where to try it now, tips for better results, and why speed matters for creative pros.]]></description>
    <pubDate>Mon, 01 Sep 2025 15:49:19 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/grok_code_fast_1_zero_lag.mp3" length="3825504" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756741759524</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+grok_code_fast_1_zero_lag.jpg"/>
  </item>

  <item>
    <title>Google Vids Superpowers: AI B-Roll, Avatars, and Speed</title>
    <description><![CDATA[Google Vids just leveled up! We break down the new AI features: fast image-to-video b-roll, on-brand AI presenters, and transcript magic, all inside Google Workspace. Dive into how creators, marketers, and teams can storyboard, auto-edit, and publish videos quicker—no pro skills required. Hear workflow shortcuts, big-picture impacts, and why Google is pushing video-first collaboration. Get ready to power up your content game!]]></description>
    <pubDate>Tue, 02 Sep 2025 18:28:17 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/google_vids_ai_boost.mp3" length="3573000" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756837697227</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+google_vids_ai_boost.jpg"/>
  </item>

  <item>
    <title>Go Bananas for On-Model AI Art in Descript!</title>
    <description><![CDATA[Descript’s new “nano banana” image model keeps your characters and products looking consistent across edits. Learn why creators, marketers, YouTubers, and brand designers are obsessed—plus how Google’s watermarking ups trust and safety. From faster thumbnails to brand-safe ads, Blue Lightning AI Daily unpacks this delicious update and introduces the consistency wave rolling through creative AI tools.]]></description>
    <pubDate>Wed, 03 Sep 2025 16:29:11 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/bananas_for_consistency.mp3" length="3552648" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756916951036</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+bananas_for_consistency.jpg"/>
  </item>


  <item>
    <title>Google Photos Create Tab: Motion Magic Unleashed</title>
    <description><![CDATA[Discover Google Photos’ new Create tab and the Veo 3-powered Photo-to-Video upgrade. We break down fresh Remix styles, cinematic animations, and highlight videos—all in-app. Find out how creators can batch slick stories faster with AI, tap into highlights from old archives, and skip the extra editing hassle. Pro tips included for eye-catching, scroll-stopping visuals. Turn memories into motion—no excuses!]]></description>
    <pubDate>Fri, 05 Sep 2025 22:37:26 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/google_photos_magic.mp3" length="3201240" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757111846421</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+google_photos_magic.jpg"/>
  </item>


  <item>
    <title>Alibaba Qwen3‑Max‑Preview: Trillion-Parameter AI for Creators</title>
    <description><![CDATA[On this episode of Blue Lightning AI Daily, Zane and Pippa dive into Alibaba’s new Qwen3‑Max‑Preview, a trillion-plus parameter large language model making waves for content creators. Is it just big model hype, or does scale translate to workflow wins? Discover how Qwen3‑Max‑Preview offers improved instruction following, long multi-turn chats, and powerful agentic features for planning and tool use. Learn how creators from daily YouTubers and podcasters to filmmakers and brand agencies can benefit from longer context windows, more reliable outputs, and streamlined content pipelines. We break down Alibaba’s pitch versus competitors, pricing insights, and where this model fits in the arms race with OpenAI, Google, and Anthropic. Plus, hear hands-on scenarios for creative pros and hobbyists, pricing comparisons, and warnings about preview limitations. Whether you run a pro studio or create on a shoestring budget, discover how Qwen3‑Max‑Preview changes the game for AI-powered content. Tune in for everything you need to know about this bold new entry in the 2025 AI landscape.]]></description>
    <pubDate>Sat, 06 Sep 2025 17:13:54 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250906130359338+-+qwen3_max_preview_ai.mp3" length="4877424" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757178834891</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250906130359338+-+qwen3_max_preview_ai.jpg"/>
  </item>

  <item>
    <title>Vidu Q1 Drops Seven-Image Continuity for AI Video</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we break down ShengShu Technology’s latest Vidu Q1 update: multi-reference video generation. Imagine keeping your characters, props, and backgrounds consistent across a whole sequence—not just a shot—using up to seven reference images. Vidu Q1’s Reference-to-Video workflow aims to crush the classic problems like disappearing hats and morphing hero products, letting creators lock in continuity and reduce time lost to patchwork fixes. We compare it with rivals like Runway, Pika, and Luma and explain how more references mean fewer weird wardrobe jumps or identity drift. Find out why this helps everyone from YouTube regulars to ad agencies, plus solo filmmakers tired of reshoots and cleanup. We cover new features like 'First-to-Last Frame' seamless transitions and API-powered automation for brands. Hear practical workflows for character scenes and branded product shots, plus vital tips for prepping references and matching lighting. We also flag limitations like over-rigid faces and ambiguous input images, explain pricing and availability, and dig into how the update fits the evolving battle for AI video control. Will this replace your whole toolkit? Not yet. But for continuity, Vidu Q1’s seven-ref system might be your secret weapon. Tune in for the meme send-off and practical advice you can use today.]]></description>
    <pubDate>Mon, 08 Sep 2025 21:32:32 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250908171755570+-+vidu_q1_seven_refs_update.mp3" length="3828984" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757367152301</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250908171755570+-+vidu_q1_seven_refs_update.jpg"/>
  </item>

  <item>
    <title>Apple vs Pixel: AI-Powered Phones Go Native</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, Zane and Pippa break down the seismic shift in mobile AI as Apple and Google roll out radical new features. Apple’s iPhone event showcased Apple Intelligence—a generative AI layer baked right into your device, with privacy at its core and slick workflow upgrades like Writing Tools and smarter Siri. Meanwhile, Google’s Pixel 9 drops Gemini everywhere: from voice summaries to Docs to fast image gen, all woven across Android and the cloud. We compare Apple’s privacy-first on-device AI with Google’s “AI mesh” that syncs across your digital life. Whether you’re a YouTuber, podcaster, designer, or writer, we dig into real creator use cases—like instant script rewrites, visual thumbnails, clean transcriptions, fast collaboration, and obsessive privacy controls. Which approach saves more time? What about battery life, data safety, and regional rollouts? Plus, we unpack what these moves mean for the whole app ecosystem, pro tools, and that growing subscription subtext around AI. Are you Team Continuity or Team Cloud Mesh? Tune in for the ultimate iPhone vs Pixel AI vibe check—because wherever you land, creators finally win as AI becomes the default layer, not just another app.]]></description>
    <pubDate>Tue, 09 Sep 2025 22:44:54 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250909182003491+-+apple_vs_pixel_ai.mp3" length="4323672" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757457894366</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250909182003491+-+apple_vs_pixel_ai.jpg"/>
  </item>

  <item>
    <title>Nvidia Rubin CPX: AI That Remembers Your Entire Movie</title>
    <description><![CDATA[Ready for AI that never forgets—and keeps your edits in sync? Today we unpack Nvidia’s new Rubin CPX inference chip, built just for creative pros who need their AI to recall every beat of a scene or line of code. Rubin CPX is all about massive context windows: million-token, hour-scale workflows where generative tools never lose the thread.

Zane and Pippa run through the hardware hype: 30 PFLOPs of power, 128 GB of rapid memory, and on-chip video encode and decode. The magic? Rubin CPX fuses media engines with transformer inference right on silicon, so long-form audio, video, dubbing, and code analysis all stay coherent—no more forgetting that intro gag or missing a key plot twist.

Hear why data centers and creative tools are racing to adopt it, and what that means for YouTubers, podcasters, TikTokers, and filmmakers itching for continuous AI-powered workflows. Learn how “context lock” and “style sync” might finally work across your full project, not just short clips. Plus: We break down Nvidia’s bold revenue claims, rollout timeline, and how new workflows could replace tedious chunking and stitching for anyone creating hour-plus content.

Get the scoop on specs, architecture, competitive landscape, and the practical impact for creators and studios. If you’re dreaming of AI that holds the whole story together, Rubin CPX is the tech to watch.]]></description>
    <pubDate>Thu, 11 Sep 2025 15:18:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250911093026288+-+nvidia_rubin_cpx_pod.mp3" length="5063496" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757603903251</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250911093026288+-+nvidia_rubin_cpx_pod.jpg"/>
  </item>

  <item>
    <title>Synamedia’s All-In AI: Smarter Streams and Highlights</title>
    <description><![CDATA[Today we break down Synamedia’s headline-grabbing AI rollout at IBC2025, which brings artificial intelligence to every corner of the streaming and broadcast workflow. From conversational search that lets you instantly find moments like "Messi’s third goal" to live tagging that auto-flags game highlights as they happen, this is a serious upgrade for anyone working with live or on demand video. Synamedia bakes in dynamic creative personalization, contextual ad intelligence, and even an AI video quality agent that tunes encoders on the fly—saving time, money, and network headaches. Quortex Switch debuts in Europe, promising standards-based multi-CDN switching with smarter, real-time routing decisions, while ContentArmor watermarking keeps content secure. We cover who benefits most (think broadcasters, sports streamers, rights holders, and pro creator teams), and why even small creators will feel the ripple effect as platforms get these upgrades. Plus, insights into the industry move toward open standards like OpenMOQ and Media over QUIC for better real-time streaming. If you care about clips, live highlights, quality, or smarter delivery, this episode unpacks the future of broadcast AI—beyond demos, these are workflow game-changers rolling out now.]]></description>
    <pubDate>Thu, 11 Sep 2025 18:05:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250911123526354+-+synamedia_ai_stack.mp3" length="4149456" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757613923235</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250911123526354+-+synamedia_ai_stack.jpg"/>
  </item>

  <item>
    <title>Seedream 4.0: 4K Image AI That Means Business</title>
    <description><![CDATA[Ready for true 4K AI images in any dimension and workflow? Today Zane and Pippa break down Seedream 4.0 from ByteDance, the latest contender in the AI image generator world. Seedream 4.0 brings genuine 4K output, custom aspect ratios, and an API built for creators and teams who care about precision, speed, and scalable workflows. We dig into why the exact-pixel control is a game changer for YouTubers, brands, podcasters, and design studios that need reliable asset generation without pixel drama. With claims of being over ten times faster than the last version, plus cost-effective direct pricing at just $0.03 per image, Seedream is positioning itself as perfect for large campaigns and creators who need consistent, production-ready batches. We talk about hands and faces (yep, improved!), lighting coherence, and multi-image orchestrations that save precious hours of cleanup and cropping. We also size up Seedream 4.0 against rivals like Nano Banana, Midjourney, and Ideogram—unpacking ELO benchmarks, versatility, and why this tool feels less like a toy and more like a production engine. Pippa and Zane run through rapid-fire use cases, workflow wins, and practical money-saving tips. Whether you are a solo creative, content team, or developer wiring into Seedream’s batch API, this episode has the need-to-know on today’s fastest and sharpest generative art engine. Is it a must-have or just nice-to-have? Find out, plus learn where and how to get started.]]></description>
    <pubDate>Sat, 13 Sep 2025 15:33:03 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250913110529795+-+seedream_4_0_4k.mp3" length="3578040" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757777583063</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250913110529795+-+seedream_4_0_4k.jpg"/>
  </item>

  <item>
    <title>YouTube Drops Veo 3 Fast and Speech to Song for Shorts</title>
    <description><![CDATA[YouTube is supercharging content creation with a trio of new AI features dropping right into Shorts: Veo 3 Fast for rapid prompt-to-video generation, Edit with AI for instant draft edits, and Speech to Song that morphs lines of dialogue into catchy musical hooks. On this episode, we break down how Veo 3 Fast lets you spin up 480p clips with audio in seconds, right from the Shorts camera or YouTube Create app. Edit with AI scans your raw footage, drafts a highlight reel, sequences edits, adds music, and even provides voice narration, all in a tap or two. We also dive into Speech to Song, powered by DeepMind’s Lyria 2, which lets creators turn spoken lines into shareable audio hooks complete with auto-attribution. Find out which creators benefit most, how YouTube’s integrated approach compares to TikTok and third-party tools like CapCut or Runway, and what it means for your posting workflow. Plus, updates on YouTube’s policies for AI content, watermarks, and deepfake protection, as well as rollout timing, limitations, and where these features are available first. Discover how these tools help brands, solo creators, podcasters, and even filmmakers rethink their process—saving hours and unlocking new ways to experiment and share ideas instantly on Shorts.]]></description>
    <pubDate>Tue, 16 Sep 2025 18:24:58 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250916141337713+-+youtube_veo_3_fast_ai.mp3" length="3402168" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758047098824</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250916141337713+-+youtube_veo_3_fast_ai.jpg"/>
  </item>

  <item>
    <title>ElevenLabs Studio 3.0: A Timeline Revolution for Creators</title>
    <description><![CDATA[ElevenLabs Studio 3.0 is here and it is shaking up the game for podcasters, YouTubers, audiobook creators, and agencies. In this episode, we dive deep into everything new: true video support, seamless voiceovers and music beds, automatic multilingual captions, and the headline feature, Speech Correction—just edit your text and your own cloned voice matches right up, no more closet re-records. Now, creators can upload video, mix in narration, underscoring, and sound effects, all on a single browser-based timeline. It is designed to cut out app-hopping and let your creative process flow from scriptwriting to finished media in one place. We compare Studio 3.0 to Descript, CapCut, and Adobe, and unpack why ElevenLabs stands out with its elite text-to-speech technology and integrated music and SFX. Who benefits most from this upgrade? From solo TikTokers and podcast producers to agencies managing client feedback, Studio 3.0 speeds up workflows and simplifies feedback and revisions. Multilingual support is a sleeper hit, opening doors to new audiences without extra tools. We break down the pros, cons, and limitations, including free tier restrictions and browser performance. For anyone in voice-led media, Studio 3.0 might just replace half your current tool stack. Tune in for all the practical use cases, pricing tips, and the future of voice-first production.]]></description>
    <pubDate>Wed, 17 Sep 2025 18:15:03 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250917135710858+-+elevenlabs_studio_3.mp3" length="4261584" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758132903023</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250917135710858+-+elevenlabs_studio_3.jpg"/>
  </item>

  <item>
    <title>Zoom Drops Photoreal Avatars and Cross-Platform AI Notes</title>
    <description><![CDATA[Zoom just shook up video meetings at Zoomtopia with AI Companion 3.0, launching November 2025 for paid Workplace users at no extra cost. The update packs a punch: photorealistic avatars that mimic your real expressions arrive in December, offering a polished on-camera presence for when you want to skip the makeup or hit a meeting without being camera-ready. More than just looking real, these avatars could change the culture of remote work—with norms around disclosure sure to follow.

But the feature everyone’s buzzing about? AI Companion’s new cross-platform notes. Now, Zoom’s slick note-taking follows you not just in Zoom, but across Microsoft Teams and Google Meet too. That means automatic summaries, action items, and follow-ups no matter which platform you or your clients prefer, slashing busywork and context-switching for agencies, creators, educators, and podcasters alike.

Zoom 3.0 ups the fidelity too: 60fps video meetings, 1080p and 4K sharing hit Zoom Rooms for smoother design reviews, crisper content walkthroughs, and vibrant classroom sessions. “Free up my time” agentic features debut as well, with proactive nudges to keep you focused and drop low-value calls, moving Zoom’s AI from passive notetaker to active meeting coordinator.

The competition is fierce, with Microsoft, Google, and Cisco all dropping AI-powered meeting tools and avatars. But Zoom’s cross-platform notes and photoreal avatars are a bold play, aiming for both style and serious productivity gains. Expect companies to clarify policies on avatar use and watch how deep Zoom’s integration really goes—but if the rollout lives up to the hype, your next meeting might just star your digital twin and a smarter notetaker.]]></description>
    <pubDate>Fri, 19 Sep 2025 20:11:25 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250919160158448+-+zoom_ai_companion_3.mp3" length="3548136" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758312685004</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250919160158448+-+zoom_ai_companion_3.jpg"/>
  </item>

  <item>
    <title>Lucy Edit Dev: AI Video Edits with One Command</title>
    <description><![CDATA[Get ready for your fastest video edit yet. Today, Zane and Pippa break down Lucy Edit Dev from Decart AI—an open-weight, instruction-following video editor now on Hugging Face. Unlike text-to-video generators, Lucy Edit Dev lets you edit existing footage with natural language prompts: swap a kimono, replace a background, or turn a barista into an astronaut, all without painstaking masking. Powered by a VAE and Diffusion Transformer backbone in the tradition of Wan 2.2, Lucy Edit Dev keeps shot composition, motion, and timing stable while delivering precise instruction-driven edits. It is dev-focused, open for research and R and D, but under a non-commercial license, making it perfect for prototyping, benchmarking, or personal creative exploration. Find out why creators, marketers, designers, and researchers are excited about this new “do what I said” tool for efficient A B testing and visual experimentation. We cover setup needs, real-world uses, limitations like GPU compute, edge cases and license restrictions, plus how Lucy Edit Dev stacks up against Runway, Pika, Luma, and Adobe’s video AI tools. Whether you are tired of tedious rotoscoping or want to quickly generate multiple style options for your YouTube video, discover if Lucy Edit Dev unlocks a new layer of creative control and speed. Listen in for takeaways, practical tips, and the tagline memes you did not know you needed.]]></description>
    <pubDate>Tue, 23 Sep 2025 00:41:02 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250922195704275+-+lucy_edit_dev_hf.mp3" length="3556896" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758588062872</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250922195704275+-+lucy_edit_dev_hf.jpg"/>
  </item>

  <item>
    <title>Qwen3-Omni: Real-Time Multimodal AI Goes Open Source</title>
    <description><![CDATA[Can AI really respond in under a second and handle text, video, speech, and images—all at once? Meet Qwen3-Omni from Alibaba’s Qwen team. This open-source, Apache 2.0-licensed model combines multimodal understanding with real-time, streaming speech. Qwen3-Omni features a clever split: the Thinker does the smart perception, while the Talker delivers lightning-fast voice feedback, shrinking typical multi-tool workflows into one live loop. The big advantage? Sub-second response times reported as low as 211 milliseconds, open weights, and legal clarity for commercial use. Whether you’re a YouTuber wanting express captions, a podcaster making global episodes, or a developer building real-time agents, Qwen3-Omni drops speed and versatility where others gatekeep. It stands out from closed rivals like GPT-4o Realtime and Google’s Astra, and even edges out open options such as SeamlessM4T with less restrictive licensing. In today’s episode, discover how Qwen3-Omni can tighten creator workflows, sharpen media searches with OCR and Vision Q&A, and give you full control over data and deployment. We break down practical use cases—for video, podcasting, design, and even Twitch streams—and reality check the claims around speed and model size. If you’re building anything voice-interactive or content-smart, Qwen3-Omni’s all-in-one approach could change your pipeline and maybe even your budget.]]></description>
    <pubDate>Tue, 23 Sep 2025 13:55:54 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250923092902312+-+qwen3_omni_multimodal.mp3" length="3111696" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758635754698</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250923092902312+-+qwen3_omni_multimodal.jpg"/>
  </item>

  <item>
    <title>Suno v5: AI Music Goes Pro with Studio-Ready Sound</title>
    <description><![CDATA[Blue Lightning AI takes you inside Suno v5, the latest AI music model delivering release-ready tracks with lifelike vocals, cleaner mixes, and tighter creative control. Explore how Suno v5 raises the bar on audio fidelity, arrangement, and prompt accuracy. Hear about its game-changing remaster feature, which lets you upgrade old tracks with faithful or adventurous new versions. Compare Suno to Udio and learn why v5 could streamline workflows for creators, podcasters, marketers, and musicians. Get the scoop on paid-only access, value calculation, and upcoming features hinted by Suno's WavTool acquisition. Discover real-world scenarios from TikTok hooks to pro client projects, plus a candid breakdown of current pros and cons. This is your fast guide to deciding if Suno v5 is the AI music upgrade you have been waiting for.]]></description>
    <pubDate>Wed, 24 Sep 2025 11:29:49 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250924062448156+-+suno_v5_music_upgrade.mp3" length="2924088" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758713389659</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250924062448156+-+suno_v5_music_upgrade.jpg"/>
  </item>

  <item>
    <title>Kling AI 2.5 Turbo: No More Wobbly Shots</title>
    <description><![CDATA[Kling AI just launched its 2.5 Turbo Video model and it is a true game changer for creators. Say goodbye to drifting camera shots and random style changes. This update brings tighter prompt fidelity, reduced jitter and flicker, and huge improvements in character and motion stability. Whether you produce ads, music visuals, or social campaigns, Kling 2.5 Turbo gives you cleaner frames and fewer do-overs for faster workflows. With aspect ratios for every platform, 30 fps, and up to 1080p on pro accounts, it fits vertical and widescreen projects alike. Costs are down by around 30 percent per shot over version 2.1, so you can experiment more without burning credits. Access it now in the Kling app or request the API if you want to automate your pipeline. Need steady shots for client work or music videos? The new model actually keeps hair color, backgrounds, and objects consistent between frames and scenes. Drag and drop your references for multi-angle consistency and enjoy smoother handoffs to post production. If you want tools that just work, Kling 2.5 Turbo moves AI video forward from flashy demos to reliable daily production. Bonus: the platform race with ByteDance, OpenAI, and Runway is heating up fast. For creators, that means smarter options, simpler workflows, and the ability to hit deadlines with less stress. Listen in for tips on getting the most out of the new version and why this update is all about fewer reshoots and more creativity.]]></description>
    <pubDate>Thu, 25 Sep 2025 19:23:48 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250925150959828+-+kling25_turbo_release.mp3" length="3898320" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758828228323</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250925150959828+-+kling25_turbo_release.jpg"/>
  </item>

  <item>
    <title>Alibaba Wan 2.5-Preview: All-in-One A/V AI Creation</title>
    <description><![CDATA[Dive into Alibaba's Wan 2.5-Preview, the breakthrough model claiming the crown for unified multimodal generation. Zane and Pippa break down what makes this model unique: native support for text, images, video, and audio in perfect sync—no more bouncing between editing tools or struggling to line up sound to visuals. Hear how creators, marketers, and designers can generate beat-synced short videos, animated cover art, or flawlessly branded product shots all within a single model. Learn how real-time reference images, audio stems, and precise prompts allow advanced control over pacing, narration, SFX, and style. Compare Wan 2.5 to OpenAI’s Sora, Runway Gen-3, and Google’s Veo 3 as the crew discusses who wins in the new race for native audio-video generation. The episode covers how this API-first tool changes workflow efficiency, identity preservation, motion branding, and product visualization—plus practical limits, pricing, and region availability for creatives and teams. From TikTok and YouTube Shorts to next-level product videos, discover why Alibaba’s 'all-in-one A/V AI' might change how you generate and sync content forever.]]></description>
    <pubDate>Fri, 26 Sep 2025 10:06:25 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250926050818058+-+alibaba_wan_25_av_sync.mp3" length="3831816" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758881185010</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250926050818058+-+alibaba_wan_25_av_sync.jpg"/>
  </item>

  <item>
    <title>Meta Vibes: AI Video Creation Goes Feed-Native</title>
    <description><![CDATA[Today we break down Meta’s ambitious new feature: Vibes. This end-to-end AI video creation and publishing feed lets you prompt, remix, edit, and post bite-sized videos directly from the Meta AI app or web, blending creation and distribution in one seamless loop. Unlike other AI video tools that stop at exporting, Vibes lets you go live with a single tap and instantly push clips to Instagram or Facebook Stories and Reels. We compare Vibes to rivals like TikTok Symphony, YouTube Dream Screen, Runway, and Pika to see who’s best positioned for the coming age of AI-native short videos. We cover how remixing and trend chaining work inside Vibes, what the workflow means for creators and brands, and the pros and cons of this all-in-one approach. Will the feed surface hit trends, or just flood with filler content? How does built-in attribution, AI music, and cross-platform posting change the game for brands and hobbyists alike? What are the early risks, quality questions, and dealbreakers to watch? Tune in for a fast-paced, glitch-free (or maybe not) look at how Meta’s Vibes could change speed, style, and strategy for anyone chasing short-form video in 2025.]]></description>
    <pubDate>Fri, 26 Sep 2025 20:09:40 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250926153906971+-+meta_vibes_ai_feed.mp3" length="3654384" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758917380091</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250926153906971+-+meta_vibes_ai_feed.jpg"/>
  </item>

  <item>
    <title>Sora 2 Debuts: One-Take Audio and Video AI Magic</title>
    <description><![CDATA[OpenAI’s Sora 2 has landed, launching as an invite-only iOS app and promising to reinvent short-form creation with synchronized audio and video in just one step. Today, Zane and Pippa break down what creators need to know: Sora 2 unifies visuals and sound, letting you generate flawless 10-second clips complete with music, dialogue, ambience, and effects all at once. Forget scrambling for soundtracks or wrangling multiple apps. Instead, TikTokers, YouTubers, marketers, and designers can whip up pro-feeling clips as fast as they can come up with prompts. 
 
How does Sora 2 compare? Google’s Veo 3 offers similar features, but is aimed at enterprises through Vertex AI, while Alibaba's Wan 2.5-Preview puts open, customizable tools in your hands. Sora 2 is a creator-first experience with a built-in feed and algorithmic reach—think more social, less back-end integration. 
 
Access is tightly controlled at launch: no API, strict moderation, and public-figure blocking, with short clips and a TikTok-style feed the early norm. If you’re a brand or agency needing hundreds of variants, Sora 2 is an experimental playground for now. For solo creators, it is an instant cheat code for cutting production time and impressing audiences. 
 
Tune in for a speed-run of Sora 2’s strengths, trade-offs, policy guardrails, and what it means for the future of AI-powered content creation.]]></description>
    <pubDate>Tue, 30 Sep 2025 19:52:12 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250930151315097+-+sora2_unified_av_magic.mp3" length="4078944" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759261932027</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250930151315097+-+sora2_unified_av_magic.jpg"/>
  </item>

  <item>
    <title>Claude Sonnet 4.5: AI That Actually Gets Work Done</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we break down Anthropic's Claude Sonnet 4.5—an upgrade that is less about hype and more about major workflow wins for creators, marketers, podcasters, and anyone managing projects. Discover how Sonnet 4.5 is leading key benchmarks like OSWorld for computer use and SWE-bench Verified for real-world coding challenges. New features let the model autonomously run agent tasks for up to 30 hours, navigate apps and the web more smoothly, and recover work with new checkpoints and instant rollbacks. Hear how the Chrome extension, Agent SDK, and sharper task memory minimize context drift and grunt work for creators of all types—whether you are wrangling YouTube footage, updating podcast show notes, or managing design assets. We compare Sonnet 4.5 to GPT-5 and Gemini 2.5, explain pricing, and get real about where it shines or might stumble (hello, CAPTCHAs and flaky web UIs). If you want AI that does more than just “chat,” and actually moves your tasks forward, tune in for a rapid-fire tour of what’s new, what’s hype-free, and how Sonnet 4.5 could save hours from your week.]]></description>
    <pubDate>Wed, 01 Oct 2025 19:32:16 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251001150339399+-+claude_sonnet_4_5_launch.mp3" length="4017960" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759347136227</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251001150339399+-+claude_sonnet_4_5_launch.jpg"/>
  </item>

  <item>
    <title>Tencent Open-Sources HunyuanImage-3.0: Game On</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, Zane and Pippa dive into the groundbreaking release of HunyuanImage-3.0 from Tencent. This new text-to-image model is fully open-sourced—including weights and inference code—offering creators unparalleled control in visual generation. Unlike previous diffusion models like Stable Diffusion or FLUX, HunyuanImage-3.0 is natively multimodal, blending text and image reasoning into one fluid stream for improved composition and prompt fidelity.

We break down how HunyuanImage-3.0’s 80-billion-parameter Mixture-of-Experts architecture delivers stable, on-brand visuals for both solo creators and large studios. The episode covers use cases from design agencies needing private deployments to YouTubers, filmmakers, podcasters, and TikTokers eager for consistent visual style. There are caveats: rendering perfect in-image text remains a challenge, hardware requirements are steep, and the license imposes limits—especially for those scaling projects or working in certain jurisdictions.

Plus, hear how the Instruct variant promises fewer prompt revisions, why the open community will accelerate plugin and integration support, and what this means for competitors like Midjourney and DALL·E. Zane and Pippa give practical advice for running the model locally, explain the cost tradeoffs for creators, and share their editorial verdicts after hands-on testing. Whether you want creative freedom, on-prem compliance, or just to experiment with the next wave in open AI imagery, this episode unpacks everything you need to know about HunyuanImage-3.0.]]></description>
    <pubDate>Fri, 03 Oct 2025 01:38:44 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251002211630679+-+tencent_hunyuanimage3.mp3" length="3505872" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759455524084</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251002211630679+-+tencent_hunyuanimage3.jpg"/>
  </item>

  <item>
    <title>OpenAI DevDay 2025: Apps, Agents, Sora 2 &amp; GPT‑5 Pro</title>
    <description><![CDATA[Dive into the biggest announcements from OpenAI’s DevDay 2025 with Zane and Pippa. We break down how ChatGPT is evolving from a single assistant into a true creative platform. Discover how new Apps in ChatGPT end constant tab-switching with one-stop creative tools, and how the Agents SDK gives teams auditable, policy-aware automation. Creators, coders, and marketers will all want the scoop on Sora 2’s next-gen video capabilities, the lightning-fast GPT-realtime-mini voice features, and the returns of the Codex code model family. We cover the strategic edge of deploying domain-specific AI in your workflow, what features to try first, new pricing clues, and crucial safety guardrails for brand-safe production. Compare OpenAI’s new offerings to Google Gemini, Anthropic, Runway Gen-3, Pika, and Adobe Firefly, and get practical ideas for YouTubers, podcasters, indie filmmakers, and design teams. If you want to work faster, automate smarter, and build with the latest AI from the ground up, don’t miss this rapid recap of Apps, Agents, Sora 2, GPT-image-1-mini, Realtime-mini, Codex, and the mighty GPT‑5 Pro.]]></description>
    <pubDate>Mon, 06 Oct 2025 21:41:11 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251006165257756+-+openai_devday_2025_recap.mp3" length="5070096" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759786871018</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251006165257756+-+openai_devday_2025_recap.jpg"/>
  </item>

  <item>
    <title>gpt-image-1-mini: The Creator’s Fast Lane for Cheap Visuals</title>
    <description><![CDATA[OpenAI just dropped gpt-image-1-mini, and it is the new daily driver for creators who need images fast and on a budget. In this episode, Zane and Pippa break down mini’s biggest selling points: lightning-quick image generation, affordable API rates, and creator-friendly features like text-to-image, image-to-image, inpainting, and style guidance via reference images. With fixed resolution options and flexible quality tiers, this model is built to keep costs predictable while delivering enough polish for YouTubers, social teams, ecom, and indie filmmakers.

We compare gpt-image-1-mini’s pricing to alternatives like Midjourney, Stable Diffusion, and Adobe Firefly. The verdict: gpt-image-1-mini is not your showpiece tool, but it is the MVP for bulk content, batch iterations, and last mile edits when “good enough” and “on time” are the goals. We walk through real-world workflows for batch thumbnails, ad variants, product mockups, and storyboard frames. The bottom line: you can test 100 ideas for just a few bucks, hang at a beginner level with simple prompts and masks, or tune things up with more advanced features if you are a pro.

Hear about practical limitations, community experiments, and where mini fits in the new landscape of API-first, budget-forward image models. If you want a reliable, plug-and-play image generator for fast creative projects, mini could be your new favorite tool. Stay tuned for real use cases, honest takes, and fun analogies from the field.]]></description>
    <pubDate>Wed, 08 Oct 2025 19:46:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251008145348464+-+gpt_image_1_mini_daily.mp3" length="3935856" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759952783766</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251008145348464+-+gpt_image_1_mini_daily.jpg"/>
  </item>

  <item>
    <title>Sora 2 Joins Artlist: All-in-One Video Creation Powerup</title>
    <description><![CDATA[Big news for creators: Sora 2 is now live inside Artlist, meaning instant access to high-fidelity AI video generation without chasing an OpenAI invite. This episode unpacks what makes the Artlist x Sora 2 launch a game changer for YouTubers, marketers, agencies and solo makers. Consolidate video gen, stock, SFX, voiceover, and licensed music under one roof. We break down how Artlist’s credits system works across different plans, and why bundling these tools means faster pitching, fewer legal headaches, and less tool-juggling. Hear which creators and teams will benefit most, plus how it stacks up against other platforms like Google Veo 3 with Flow, Runway, and Adobe’s ecosystem. We get into practical examples: from TikTok product drops to filmmaker pre-visualization and pitch frames—all with commercial licensing covered, no more rights-chasing marathons. Learn the trade-offs, dealbreakers, and who should keep an eye on feature updates. The future of creator-friendly, production-legal AI video is here, and Sora 2 in Artlist is setting a new bar.]]></description>
    <pubDate>Sun, 12 Oct 2025 22:58:44 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251012162929884+-+sora2_artlist_creators.mp3" length="2891184" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1760309924463</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251012162929884+-+sora2_artlist_creators.jpg"/>
  </item>

  <item>
    <title>Grok Imagine Turbocharges AI Video with Multi-Render</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we dive into xAI’s Grok upgrades shaking up short-form video creation. The new Grok Imagine delivers parallel variants from a single prompt, speeding content creation for TikTokers, agencies, and YouTubers. The highly anticipated “Eve” voice gives a natural, human-like narration right in the app, reducing the need for third-party TTS tools. xAI’s push is toward lightning-fast draft-to-publish workflow: create synced six-second clips, batch test promo hooks, and never fuss with exporting audio again. We break down who benefits most from these upgrades, from solo creators to brand teams, and discuss the new “spicy” mode’s built-in moderation filters. Plus, learn how xAI’s “world models” tease could transform indie game development by 2026 and what Macrohard might mean for automated coding. Stay tuned for a full competitive landscape, pricing rundown, and why Grok’s mobile-first speed and built-in voice features set it apart from OpenAI’s Sora, Google’s Veo, and more. Our hosts explain how Grok Imagine’s speed and variety can reduce tool-hopping, increase creative throughput, and change the AI video game. Grab your coffee and join us as we cover the latest in creator-centric AI tools.]]></description>
    <pubDate>Tue, 14 Oct 2025 01:40:48 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251013211613688+-+grok_imagine_multirender.mp3" length="3646392" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1760406048287</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251013211613688+-+grok_imagine_multirender.jpg"/>
  </item>
</channel>
</rss>
