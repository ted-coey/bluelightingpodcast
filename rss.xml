<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
     xmlns:atom="http://www.w3.org/2005/Atom">

  <channel>
    <title>Blue Lightning AI Daily</title>
    <link>https://yourdomain.com/podcast</link>
    <language>en-us</language>
    <copyright>© 2025 Blue Lightning</copyright>
    <lastBuildDate>Wed, 28 Jan 2026 17:44:51 GMT</lastBuildDate>
    <pubDate>Sun, 31 Aug 2025 12:00:00 GMT</pubDate>
     <itunes:category text="Business">
       <itunes:category text="Entrepreneurship"/>
     </itunes:category>
     <itunes:category text="Technology">
       <itunes:category text="Tech News"/>
     </itunes:category>
    <generator>RSS Builder</generator>
    <description>
      Blue Lightning AI Daily is your go-to AI podcast for creators, delivering fast, focused updates on the world of generative AI. We cover the latest breakthroughs in large language models (LLMs), AI video editing, AI photography, AI audio tools, and creative automation. Each episode gives digital creators, content marketers, and creative professionals clear insights into how AI is transforming storytelling, production, and the creator economy. Stay informed, stay creative, and stay ahead with daily AI news made simple.
    </description>

    <!-- iTunes / Apple Podcasts tags -->
    <itunes:author>Blue Lightning</itunes:author>
    <itunes:summary>
      Blue Lightning AI Daily is your go-to AI podcast for creators, delivering fast, focused updates on the world of generative AI. 
      We cover the latest breakthroughs in LLMs, AI video, AI audio, and creative tools.
    </itunes:summary>
    <itunes:explicit>no</itunes:explicit>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/blue-lighting-cover.jpg"/>
    <itunes:owner>
      <itunes:name>Blue Lightning</itunes:name>
      <itunes:email>ted@coey.com</itunes:email>
    </itunes:owner>

    <!-- Atom self-link (recommended for apps like Apple Podcasts / Overcast) -->
    <atom:link href="https://raw.githubusercontent.com/ted-coey/bluelightingpodcast/main/rss.xml" rel="self" type="application/rss+xml"/>

    <!-- Episodes will be inserted here as <item> blocks -->

  <item>
    <title>Nano Banana: The Meme-Powered Image Revolution</title>
    <description><![CDATA[Discover why creators are obsessed with “Nano Banana”—Google’s new high-consistency image tool in Gemini. Learn how stable likeness, fast multi-step edits, and watermark safeguards are changing workflows for artists, marketers, and everyday users. We break down costs, real-world uses, and pro tips for perfect prompts, plus why this fun meme-model is winning fans across the AI art community.]]></description>
    <pubDate>Sun, 31 Aug 2025 19:38:18 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/nano_banana_revolution.mp3" length="3761760" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756669098912</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+nano_banana_revolution.jpg"/>
  </item>

  <item>
    <title>Grok Code Fast 1: Blazing Agent Loops, Zero Lag</title>
    <description><![CDATA[We break down xAI’s Grok Code Fast 1, a lightning-speed coding model made for agents that build, test, and fix in sub-seconds. From creators to dev teams, learn how this low-latency engine transforms workflows, slashes context-switching, and enables affordable micro-automation. Find out where to try it now, tips for better results, and why speed matters for creative pros.]]></description>
    <pubDate>Mon, 01 Sep 2025 15:49:19 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/grok_code_fast_1_zero_lag.mp3" length="3825504" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756741759524</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+grok_code_fast_1_zero_lag.jpg"/>
  </item>

  <item>
    <title>Google Vids Superpowers: AI B-Roll, Avatars, and Speed</title>
    <description><![CDATA[Google Vids just leveled up! We break down the new AI features: fast image-to-video b-roll, on-brand AI presenters, and transcript magic, all inside Google Workspace. Dive into how creators, marketers, and teams can storyboard, auto-edit, and publish videos quicker—no pro skills required. Hear workflow shortcuts, big-picture impacts, and why Google is pushing video-first collaboration. Get ready to power up your content game!]]></description>
    <pubDate>Tue, 02 Sep 2025 18:28:17 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/google_vids_ai_boost.mp3" length="3573000" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756837697227</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+google_vids_ai_boost.jpg"/>
  </item>

  <item>
    <title>Go Bananas for On-Model AI Art in Descript!</title>
    <description><![CDATA[Descript’s new “nano banana” image model keeps your characters and products looking consistent across edits. Learn why creators, marketers, YouTubers, and brand designers are obsessed—plus how Google’s watermarking ups trust and safety. From faster thumbnails to brand-safe ads, Blue Lightning AI Daily unpacks this delicious update and introduces the consistency wave rolling through creative AI tools.]]></description>
    <pubDate>Wed, 03 Sep 2025 16:29:11 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/bananas_for_consistency.mp3" length="3552648" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756916951036</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+bananas_for_consistency.jpg"/>
  </item>


  <item>
    <title>Google Photos Create Tab: Motion Magic Unleashed</title>
    <description><![CDATA[Discover Google Photos’ new Create tab and the Veo 3-powered Photo-to-Video upgrade. We break down fresh Remix styles, cinematic animations, and highlight videos—all in-app. Find out how creators can batch slick stories faster with AI, tap into highlights from old archives, and skip the extra editing hassle. Pro tips included for eye-catching, scroll-stopping visuals. Turn memories into motion—no excuses!]]></description>
    <pubDate>Fri, 05 Sep 2025 22:37:26 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/google_photos_magic.mp3" length="3201240" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757111846421</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+google_photos_magic.jpg"/>
  </item>


  <item>
    <title>Alibaba Qwen3‑Max‑Preview: Trillion-Parameter AI for Creators</title>
    <description><![CDATA[On this episode of Blue Lightning AI Daily, Zane and Pippa dive into Alibaba’s new Qwen3‑Max‑Preview, a trillion-plus parameter large language model making waves for content creators. Is it just big model hype, or does scale translate to workflow wins? Discover how Qwen3‑Max‑Preview offers improved instruction following, long multi-turn chats, and powerful agentic features for planning and tool use. Learn how creators from daily YouTubers and podcasters to filmmakers and brand agencies can benefit from longer context windows, more reliable outputs, and streamlined content pipelines. We break down Alibaba’s pitch versus competitors, pricing insights, and where this model fits in the arms race with OpenAI, Google, and Anthropic. Plus, hear hands-on scenarios for creative pros and hobbyists, pricing comparisons, and warnings about preview limitations. Whether you run a pro studio or create on a shoestring budget, discover how Qwen3‑Max‑Preview changes the game for AI-powered content. Tune in for everything you need to know about this bold new entry in the 2025 AI landscape.]]></description>
    <pubDate>Sat, 06 Sep 2025 17:13:54 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250906130359338+-+qwen3_max_preview_ai.mp3" length="4877424" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757178834891</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250906130359338+-+qwen3_max_preview_ai.jpg"/>
  </item>

  <item>
    <title>Vidu Q1 Drops Seven-Image Continuity for AI Video</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we break down ShengShu Technology’s latest Vidu Q1 update: multi-reference video generation. Imagine keeping your characters, props, and backgrounds consistent across a whole sequence—not just a shot—using up to seven reference images. Vidu Q1’s Reference-to-Video workflow aims to crush the classic problems like disappearing hats and morphing hero products, letting creators lock in continuity and reduce time lost to patchwork fixes. We compare it with rivals like Runway, Pika, and Luma and explain how more references mean fewer weird wardrobe jumps or identity drift. Find out why this helps everyone from YouTube regulars to ad agencies, plus solo filmmakers tired of reshoots and cleanup. We cover new features like 'First-to-Last Frame' seamless transitions and API-powered automation for brands. Hear practical workflows for character scenes and branded product shots, plus vital tips for prepping references and matching lighting. We also flag limitations like over-rigid faces and ambiguous input images, explain pricing and availability, and dig into how the update fits the evolving battle for AI video control. Will this replace your whole toolkit? Not yet. But for continuity, Vidu Q1’s seven-ref system might be your secret weapon. Tune in for the meme send-off and practical advice you can use today.]]></description>
    <pubDate>Mon, 08 Sep 2025 21:32:32 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250908171755570+-+vidu_q1_seven_refs_update.mp3" length="3828984" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757367152301</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250908171755570+-+vidu_q1_seven_refs_update.jpg"/>
  </item>

  <item>
    <title>Apple vs Pixel: AI-Powered Phones Go Native</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, Zane and Pippa break down the seismic shift in mobile AI as Apple and Google roll out radical new features. Apple’s iPhone event showcased Apple Intelligence—a generative AI layer baked right into your device, with privacy at its core and slick workflow upgrades like Writing Tools and smarter Siri. Meanwhile, Google’s Pixel 9 drops Gemini everywhere: from voice summaries to Docs to fast image gen, all woven across Android and the cloud. We compare Apple’s privacy-first on-device AI with Google’s “AI mesh” that syncs across your digital life. Whether you’re a YouTuber, podcaster, designer, or writer, we dig into real creator use cases—like instant script rewrites, visual thumbnails, clean transcriptions, fast collaboration, and obsessive privacy controls. Which approach saves more time? What about battery life, data safety, and regional rollouts? Plus, we unpack what these moves mean for the whole app ecosystem, pro tools, and that growing subscription subtext around AI. Are you Team Continuity or Team Cloud Mesh? Tune in for the ultimate iPhone vs Pixel AI vibe check—because wherever you land, creators finally win as AI becomes the default layer, not just another app.]]></description>
    <pubDate>Tue, 09 Sep 2025 22:44:54 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250909182003491+-+apple_vs_pixel_ai.mp3" length="4323672" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757457894366</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250909182003491+-+apple_vs_pixel_ai.jpg"/>
  </item>

  <item>
    <title>Nvidia Rubin CPX: AI That Remembers Your Entire Movie</title>
    <description><![CDATA[Ready for AI that never forgets—and keeps your edits in sync? Today we unpack Nvidia’s new Rubin CPX inference chip, built just for creative pros who need their AI to recall every beat of a scene or line of code. Rubin CPX is all about massive context windows: million-token, hour-scale workflows where generative tools never lose the thread.

Zane and Pippa run through the hardware hype: 30 PFLOPs of power, 128 GB of rapid memory, and on-chip video encode and decode. The magic? Rubin CPX fuses media engines with transformer inference right on silicon, so long-form audio, video, dubbing, and code analysis all stay coherent—no more forgetting that intro gag or missing a key plot twist.

Hear why data centers and creative tools are racing to adopt it, and what that means for YouTubers, podcasters, TikTokers, and filmmakers itching for continuous AI-powered workflows. Learn how “context lock” and “style sync” might finally work across your full project, not just short clips. Plus: We break down Nvidia’s bold revenue claims, rollout timeline, and how new workflows could replace tedious chunking and stitching for anyone creating hour-plus content.

Get the scoop on specs, architecture, competitive landscape, and the practical impact for creators and studios. If you’re dreaming of AI that holds the whole story together, Rubin CPX is the tech to watch.]]></description>
    <pubDate>Thu, 11 Sep 2025 15:18:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250911093026288+-+nvidia_rubin_cpx_pod.mp3" length="5063496" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757603903251</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250911093026288+-+nvidia_rubin_cpx_pod.jpg"/>
  </item>

  <item>
    <title>Synamedia’s All-In AI: Smarter Streams and Highlights</title>
    <description><![CDATA[Today we break down Synamedia’s headline-grabbing AI rollout at IBC2025, which brings artificial intelligence to every corner of the streaming and broadcast workflow. From conversational search that lets you instantly find moments like "Messi’s third goal" to live tagging that auto-flags game highlights as they happen, this is a serious upgrade for anyone working with live or on demand video. Synamedia bakes in dynamic creative personalization, contextual ad intelligence, and even an AI video quality agent that tunes encoders on the fly—saving time, money, and network headaches. Quortex Switch debuts in Europe, promising standards-based multi-CDN switching with smarter, real-time routing decisions, while ContentArmor watermarking keeps content secure. We cover who benefits most (think broadcasters, sports streamers, rights holders, and pro creator teams), and why even small creators will feel the ripple effect as platforms get these upgrades. Plus, insights into the industry move toward open standards like OpenMOQ and Media over QUIC for better real-time streaming. If you care about clips, live highlights, quality, or smarter delivery, this episode unpacks the future of broadcast AI—beyond demos, these are workflow game-changers rolling out now.]]></description>
    <pubDate>Thu, 11 Sep 2025 18:05:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250911123526354+-+synamedia_ai_stack.mp3" length="4149456" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757613923235</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250911123526354+-+synamedia_ai_stack.jpg"/>
  </item>

  <item>
    <title>Seedream 4.0: 4K Image AI That Means Business</title>
    <description><![CDATA[Ready for true 4K AI images in any dimension and workflow? Today Zane and Pippa break down Seedream 4.0 from ByteDance, the latest contender in the AI image generator world. Seedream 4.0 brings genuine 4K output, custom aspect ratios, and an API built for creators and teams who care about precision, speed, and scalable workflows. We dig into why the exact-pixel control is a game changer for YouTubers, brands, podcasters, and design studios that need reliable asset generation without pixel drama. With claims of being over ten times faster than the last version, plus cost-effective direct pricing at just $0.03 per image, Seedream is positioning itself as perfect for large campaigns and creators who need consistent, production-ready batches. We talk about hands and faces (yep, improved!), lighting coherence, and multi-image orchestrations that save precious hours of cleanup and cropping. We also size up Seedream 4.0 against rivals like Nano Banana, Midjourney, and Ideogram—unpacking ELO benchmarks, versatility, and why this tool feels less like a toy and more like a production engine. Pippa and Zane run through rapid-fire use cases, workflow wins, and practical money-saving tips. Whether you are a solo creative, content team, or developer wiring into Seedream’s batch API, this episode has the need-to-know on today’s fastest and sharpest generative art engine. Is it a must-have or just nice-to-have? Find out, plus learn where and how to get started.]]></description>
    <pubDate>Sat, 13 Sep 2025 15:33:03 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250913110529795+-+seedream_4_0_4k.mp3" length="3578040" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757777583063</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250913110529795+-+seedream_4_0_4k.jpg"/>
  </item>

  <item>
    <title>YouTube Drops Veo 3 Fast and Speech to Song for Shorts</title>
    <description><![CDATA[YouTube is supercharging content creation with a trio of new AI features dropping right into Shorts: Veo 3 Fast for rapid prompt-to-video generation, Edit with AI for instant draft edits, and Speech to Song that morphs lines of dialogue into catchy musical hooks. On this episode, we break down how Veo 3 Fast lets you spin up 480p clips with audio in seconds, right from the Shorts camera or YouTube Create app. Edit with AI scans your raw footage, drafts a highlight reel, sequences edits, adds music, and even provides voice narration, all in a tap or two. We also dive into Speech to Song, powered by DeepMind’s Lyria 2, which lets creators turn spoken lines into shareable audio hooks complete with auto-attribution. Find out which creators benefit most, how YouTube’s integrated approach compares to TikTok and third-party tools like CapCut or Runway, and what it means for your posting workflow. Plus, updates on YouTube’s policies for AI content, watermarks, and deepfake protection, as well as rollout timing, limitations, and where these features are available first. Discover how these tools help brands, solo creators, podcasters, and even filmmakers rethink their process—saving hours and unlocking new ways to experiment and share ideas instantly on Shorts.]]></description>
    <pubDate>Tue, 16 Sep 2025 18:24:58 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250916141337713+-+youtube_veo_3_fast_ai.mp3" length="3402168" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758047098824</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250916141337713+-+youtube_veo_3_fast_ai.jpg"/>
  </item>

  <item>
    <title>ElevenLabs Studio 3.0: A Timeline Revolution for Creators</title>
    <description><![CDATA[ElevenLabs Studio 3.0 is here and it is shaking up the game for podcasters, YouTubers, audiobook creators, and agencies. In this episode, we dive deep into everything new: true video support, seamless voiceovers and music beds, automatic multilingual captions, and the headline feature, Speech Correction—just edit your text and your own cloned voice matches right up, no more closet re-records. Now, creators can upload video, mix in narration, underscoring, and sound effects, all on a single browser-based timeline. It is designed to cut out app-hopping and let your creative process flow from scriptwriting to finished media in one place. We compare Studio 3.0 to Descript, CapCut, and Adobe, and unpack why ElevenLabs stands out with its elite text-to-speech technology and integrated music and SFX. Who benefits most from this upgrade? From solo TikTokers and podcast producers to agencies managing client feedback, Studio 3.0 speeds up workflows and simplifies feedback and revisions. Multilingual support is a sleeper hit, opening doors to new audiences without extra tools. We break down the pros, cons, and limitations, including free tier restrictions and browser performance. For anyone in voice-led media, Studio 3.0 might just replace half your current tool stack. Tune in for all the practical use cases, pricing tips, and the future of voice-first production.]]></description>
    <pubDate>Wed, 17 Sep 2025 18:15:03 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250917135710858+-+elevenlabs_studio_3.mp3" length="4261584" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758132903023</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250917135710858+-+elevenlabs_studio_3.jpg"/>
  </item>

  <item>
    <title>Zoom Drops Photoreal Avatars and Cross-Platform AI Notes</title>
    <description><![CDATA[Zoom just shook up video meetings at Zoomtopia with AI Companion 3.0, launching November 2025 for paid Workplace users at no extra cost. The update packs a punch: photorealistic avatars that mimic your real expressions arrive in December, offering a polished on-camera presence for when you want to skip the makeup or hit a meeting without being camera-ready. More than just looking real, these avatars could change the culture of remote work—with norms around disclosure sure to follow.

But the feature everyone’s buzzing about? AI Companion’s new cross-platform notes. Now, Zoom’s slick note-taking follows you not just in Zoom, but across Microsoft Teams and Google Meet too. That means automatic summaries, action items, and follow-ups no matter which platform you or your clients prefer, slashing busywork and context-switching for agencies, creators, educators, and podcasters alike.

Zoom 3.0 ups the fidelity too: 60fps video meetings, 1080p and 4K sharing hit Zoom Rooms for smoother design reviews, crisper content walkthroughs, and vibrant classroom sessions. “Free up my time” agentic features debut as well, with proactive nudges to keep you focused and drop low-value calls, moving Zoom’s AI from passive notetaker to active meeting coordinator.

The competition is fierce, with Microsoft, Google, and Cisco all dropping AI-powered meeting tools and avatars. But Zoom’s cross-platform notes and photoreal avatars are a bold play, aiming for both style and serious productivity gains. Expect companies to clarify policies on avatar use and watch how deep Zoom’s integration really goes—but if the rollout lives up to the hype, your next meeting might just star your digital twin and a smarter notetaker.]]></description>
    <pubDate>Fri, 19 Sep 2025 20:11:25 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250919160158448+-+zoom_ai_companion_3.mp3" length="3548136" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758312685004</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250919160158448+-+zoom_ai_companion_3.jpg"/>
  </item>

  <item>
    <title>Lucy Edit Dev: AI Video Edits with One Command</title>
    <description><![CDATA[Get ready for your fastest video edit yet. Today, Zane and Pippa break down Lucy Edit Dev from Decart AI—an open-weight, instruction-following video editor now on Hugging Face. Unlike text-to-video generators, Lucy Edit Dev lets you edit existing footage with natural language prompts: swap a kimono, replace a background, or turn a barista into an astronaut, all without painstaking masking. Powered by a VAE and Diffusion Transformer backbone in the tradition of Wan 2.2, Lucy Edit Dev keeps shot composition, motion, and timing stable while delivering precise instruction-driven edits. It is dev-focused, open for research and R and D, but under a non-commercial license, making it perfect for prototyping, benchmarking, or personal creative exploration. Find out why creators, marketers, designers, and researchers are excited about this new “do what I said” tool for efficient A B testing and visual experimentation. We cover setup needs, real-world uses, limitations like GPU compute, edge cases and license restrictions, plus how Lucy Edit Dev stacks up against Runway, Pika, Luma, and Adobe’s video AI tools. Whether you are tired of tedious rotoscoping or want to quickly generate multiple style options for your YouTube video, discover if Lucy Edit Dev unlocks a new layer of creative control and speed. Listen in for takeaways, practical tips, and the tagline memes you did not know you needed.]]></description>
    <pubDate>Tue, 23 Sep 2025 00:41:02 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250922195704275+-+lucy_edit_dev_hf.mp3" length="3556896" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758588062872</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250922195704275+-+lucy_edit_dev_hf.jpg"/>
  </item>

  <item>
    <title>Qwen3-Omni: Real-Time Multimodal AI Goes Open Source</title>
    <description><![CDATA[Can AI really respond in under a second and handle text, video, speech, and images—all at once? Meet Qwen3-Omni from Alibaba’s Qwen team. This open-source, Apache 2.0-licensed model combines multimodal understanding with real-time, streaming speech. Qwen3-Omni features a clever split: the Thinker does the smart perception, while the Talker delivers lightning-fast voice feedback, shrinking typical multi-tool workflows into one live loop. The big advantage? Sub-second response times reported as low as 211 milliseconds, open weights, and legal clarity for commercial use. Whether you’re a YouTuber wanting express captions, a podcaster making global episodes, or a developer building real-time agents, Qwen3-Omni drops speed and versatility where others gatekeep. It stands out from closed rivals like GPT-4o Realtime and Google’s Astra, and even edges out open options such as SeamlessM4T with less restrictive licensing. In today’s episode, discover how Qwen3-Omni can tighten creator workflows, sharpen media searches with OCR and Vision Q&A, and give you full control over data and deployment. We break down practical use cases—for video, podcasting, design, and even Twitch streams—and reality check the claims around speed and model size. If you’re building anything voice-interactive or content-smart, Qwen3-Omni’s all-in-one approach could change your pipeline and maybe even your budget.]]></description>
    <pubDate>Tue, 23 Sep 2025 13:55:54 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250923092902312+-+qwen3_omni_multimodal.mp3" length="3111696" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758635754698</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250923092902312+-+qwen3_omni_multimodal.jpg"/>
  </item>

  <item>
    <title>Suno v5: AI Music Goes Pro with Studio-Ready Sound</title>
    <description><![CDATA[Blue Lightning AI takes you inside Suno v5, the latest AI music model delivering release-ready tracks with lifelike vocals, cleaner mixes, and tighter creative control. Explore how Suno v5 raises the bar on audio fidelity, arrangement, and prompt accuracy. Hear about its game-changing remaster feature, which lets you upgrade old tracks with faithful or adventurous new versions. Compare Suno to Udio and learn why v5 could streamline workflows for creators, podcasters, marketers, and musicians. Get the scoop on paid-only access, value calculation, and upcoming features hinted by Suno's WavTool acquisition. Discover real-world scenarios from TikTok hooks to pro client projects, plus a candid breakdown of current pros and cons. This is your fast guide to deciding if Suno v5 is the AI music upgrade you have been waiting for.]]></description>
    <pubDate>Wed, 24 Sep 2025 11:29:49 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250924062448156+-+suno_v5_music_upgrade.mp3" length="2924088" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758713389659</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250924062448156+-+suno_v5_music_upgrade.jpg"/>
  </item>

  <item>
    <title>Kling AI 2.5 Turbo: No More Wobbly Shots</title>
    <description><![CDATA[Kling AI just launched its 2.5 Turbo Video model and it is a true game changer for creators. Say goodbye to drifting camera shots and random style changes. This update brings tighter prompt fidelity, reduced jitter and flicker, and huge improvements in character and motion stability. Whether you produce ads, music visuals, or social campaigns, Kling 2.5 Turbo gives you cleaner frames and fewer do-overs for faster workflows. With aspect ratios for every platform, 30 fps, and up to 1080p on pro accounts, it fits vertical and widescreen projects alike. Costs are down by around 30 percent per shot over version 2.1, so you can experiment more without burning credits. Access it now in the Kling app or request the API if you want to automate your pipeline. Need steady shots for client work or music videos? The new model actually keeps hair color, backgrounds, and objects consistent between frames and scenes. Drag and drop your references for multi-angle consistency and enjoy smoother handoffs to post production. If you want tools that just work, Kling 2.5 Turbo moves AI video forward from flashy demos to reliable daily production. Bonus: the platform race with ByteDance, OpenAI, and Runway is heating up fast. For creators, that means smarter options, simpler workflows, and the ability to hit deadlines with less stress. Listen in for tips on getting the most out of the new version and why this update is all about fewer reshoots and more creativity.]]></description>
    <pubDate>Thu, 25 Sep 2025 19:23:48 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250925150959828+-+kling25_turbo_release.mp3" length="3898320" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758828228323</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250925150959828+-+kling25_turbo_release.jpg"/>
  </item>

  <item>
    <title>Alibaba Wan 2.5-Preview: All-in-One A/V AI Creation</title>
    <description><![CDATA[Dive into Alibaba's Wan 2.5-Preview, the breakthrough model claiming the crown for unified multimodal generation. Zane and Pippa break down what makes this model unique: native support for text, images, video, and audio in perfect sync—no more bouncing between editing tools or struggling to line up sound to visuals. Hear how creators, marketers, and designers can generate beat-synced short videos, animated cover art, or flawlessly branded product shots all within a single model. Learn how real-time reference images, audio stems, and precise prompts allow advanced control over pacing, narration, SFX, and style. Compare Wan 2.5 to OpenAI’s Sora, Runway Gen-3, and Google’s Veo 3 as the crew discusses who wins in the new race for native audio-video generation. The episode covers how this API-first tool changes workflow efficiency, identity preservation, motion branding, and product visualization—plus practical limits, pricing, and region availability for creatives and teams. From TikTok and YouTube Shorts to next-level product videos, discover why Alibaba’s 'all-in-one A/V AI' might change how you generate and sync content forever.]]></description>
    <pubDate>Fri, 26 Sep 2025 10:06:25 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250926050818058+-+alibaba_wan_25_av_sync.mp3" length="3831816" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758881185010</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250926050818058+-+alibaba_wan_25_av_sync.jpg"/>
  </item>

  <item>
    <title>Meta Vibes: AI Video Creation Goes Feed-Native</title>
    <description><![CDATA[Today we break down Meta’s ambitious new feature: Vibes. This end-to-end AI video creation and publishing feed lets you prompt, remix, edit, and post bite-sized videos directly from the Meta AI app or web, blending creation and distribution in one seamless loop. Unlike other AI video tools that stop at exporting, Vibes lets you go live with a single tap and instantly push clips to Instagram or Facebook Stories and Reels. We compare Vibes to rivals like TikTok Symphony, YouTube Dream Screen, Runway, and Pika to see who’s best positioned for the coming age of AI-native short videos. We cover how remixing and trend chaining work inside Vibes, what the workflow means for creators and brands, and the pros and cons of this all-in-one approach. Will the feed surface hit trends, or just flood with filler content? How does built-in attribution, AI music, and cross-platform posting change the game for brands and hobbyists alike? What are the early risks, quality questions, and dealbreakers to watch? Tune in for a fast-paced, glitch-free (or maybe not) look at how Meta’s Vibes could change speed, style, and strategy for anyone chasing short-form video in 2025.]]></description>
    <pubDate>Fri, 26 Sep 2025 20:09:40 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250926153906971+-+meta_vibes_ai_feed.mp3" length="3654384" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758917380091</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250926153906971+-+meta_vibes_ai_feed.jpg"/>
  </item>

  <item>
    <title>Sora 2 Debuts: One-Take Audio and Video AI Magic</title>
    <description><![CDATA[OpenAI’s Sora 2 has landed, launching as an invite-only iOS app and promising to reinvent short-form creation with synchronized audio and video in just one step. Today, Zane and Pippa break down what creators need to know: Sora 2 unifies visuals and sound, letting you generate flawless 10-second clips complete with music, dialogue, ambience, and effects all at once. Forget scrambling for soundtracks or wrangling multiple apps. Instead, TikTokers, YouTubers, marketers, and designers can whip up pro-feeling clips as fast as they can come up with prompts. 
 
How does Sora 2 compare? Google’s Veo 3 offers similar features, but is aimed at enterprises through Vertex AI, while Alibaba's Wan 2.5-Preview puts open, customizable tools in your hands. Sora 2 is a creator-first experience with a built-in feed and algorithmic reach—think more social, less back-end integration. 
 
Access is tightly controlled at launch: no API, strict moderation, and public-figure blocking, with short clips and a TikTok-style feed the early norm. If you’re a brand or agency needing hundreds of variants, Sora 2 is an experimental playground for now. For solo creators, it is an instant cheat code for cutting production time and impressing audiences. 
 
Tune in for a speed-run of Sora 2’s strengths, trade-offs, policy guardrails, and what it means for the future of AI-powered content creation.]]></description>
    <pubDate>Tue, 30 Sep 2025 19:52:12 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250930151315097+-+sora2_unified_av_magic.mp3" length="4078944" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759261932027</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250930151315097+-+sora2_unified_av_magic.jpg"/>
  </item>

  <item>
    <title>Claude Sonnet 4.5: AI That Actually Gets Work Done</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we break down Anthropic's Claude Sonnet 4.5—an upgrade that is less about hype and more about major workflow wins for creators, marketers, podcasters, and anyone managing projects. Discover how Sonnet 4.5 is leading key benchmarks like OSWorld for computer use and SWE-bench Verified for real-world coding challenges. New features let the model autonomously run agent tasks for up to 30 hours, navigate apps and the web more smoothly, and recover work with new checkpoints and instant rollbacks. Hear how the Chrome extension, Agent SDK, and sharper task memory minimize context drift and grunt work for creators of all types—whether you are wrangling YouTube footage, updating podcast show notes, or managing design assets. We compare Sonnet 4.5 to GPT-5 and Gemini 2.5, explain pricing, and get real about where it shines or might stumble (hello, CAPTCHAs and flaky web UIs). If you want AI that does more than just “chat,” and actually moves your tasks forward, tune in for a rapid-fire tour of what’s new, what’s hype-free, and how Sonnet 4.5 could save hours from your week.]]></description>
    <pubDate>Wed, 01 Oct 2025 19:32:16 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251001150339399+-+claude_sonnet_4_5_launch.mp3" length="4017960" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759347136227</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251001150339399+-+claude_sonnet_4_5_launch.jpg"/>
  </item>

  <item>
    <title>Tencent Open-Sources HunyuanImage-3.0: Game On</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, Zane and Pippa dive into the groundbreaking release of HunyuanImage-3.0 from Tencent. This new text-to-image model is fully open-sourced—including weights and inference code—offering creators unparalleled control in visual generation. Unlike previous diffusion models like Stable Diffusion or FLUX, HunyuanImage-3.0 is natively multimodal, blending text and image reasoning into one fluid stream for improved composition and prompt fidelity.

We break down how HunyuanImage-3.0’s 80-billion-parameter Mixture-of-Experts architecture delivers stable, on-brand visuals for both solo creators and large studios. The episode covers use cases from design agencies needing private deployments to YouTubers, filmmakers, podcasters, and TikTokers eager for consistent visual style. There are caveats: rendering perfect in-image text remains a challenge, hardware requirements are steep, and the license imposes limits—especially for those scaling projects or working in certain jurisdictions.

Plus, hear how the Instruct variant promises fewer prompt revisions, why the open community will accelerate plugin and integration support, and what this means for competitors like Midjourney and DALL·E. Zane and Pippa give practical advice for running the model locally, explain the cost tradeoffs for creators, and share their editorial verdicts after hands-on testing. Whether you want creative freedom, on-prem compliance, or just to experiment with the next wave in open AI imagery, this episode unpacks everything you need to know about HunyuanImage-3.0.]]></description>
    <pubDate>Fri, 03 Oct 2025 01:38:44 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251002211630679+-+tencent_hunyuanimage3.mp3" length="3505872" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759455524084</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251002211630679+-+tencent_hunyuanimage3.jpg"/>
  </item>

  <item>
    <title>OpenAI DevDay 2025: Apps, Agents, Sora 2 &amp; GPT‑5 Pro</title>
    <description><![CDATA[Dive into the biggest announcements from OpenAI’s DevDay 2025 with Zane and Pippa. We break down how ChatGPT is evolving from a single assistant into a true creative platform. Discover how new Apps in ChatGPT end constant tab-switching with one-stop creative tools, and how the Agents SDK gives teams auditable, policy-aware automation. Creators, coders, and marketers will all want the scoop on Sora 2’s next-gen video capabilities, the lightning-fast GPT-realtime-mini voice features, and the returns of the Codex code model family. We cover the strategic edge of deploying domain-specific AI in your workflow, what features to try first, new pricing clues, and crucial safety guardrails for brand-safe production. Compare OpenAI’s new offerings to Google Gemini, Anthropic, Runway Gen-3, Pika, and Adobe Firefly, and get practical ideas for YouTubers, podcasters, indie filmmakers, and design teams. If you want to work faster, automate smarter, and build with the latest AI from the ground up, don’t miss this rapid recap of Apps, Agents, Sora 2, GPT-image-1-mini, Realtime-mini, Codex, and the mighty GPT‑5 Pro.]]></description>
    <pubDate>Mon, 06 Oct 2025 21:41:11 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251006165257756+-+openai_devday_2025_recap.mp3" length="5070096" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759786871018</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251006165257756+-+openai_devday_2025_recap.jpg"/>
  </item>

  <item>
    <title>gpt-image-1-mini: The Creator’s Fast Lane for Cheap Visuals</title>
    <description><![CDATA[OpenAI just dropped gpt-image-1-mini, and it is the new daily driver for creators who need images fast and on a budget. In this episode, Zane and Pippa break down mini’s biggest selling points: lightning-quick image generation, affordable API rates, and creator-friendly features like text-to-image, image-to-image, inpainting, and style guidance via reference images. With fixed resolution options and flexible quality tiers, this model is built to keep costs predictable while delivering enough polish for YouTubers, social teams, ecom, and indie filmmakers.

We compare gpt-image-1-mini’s pricing to alternatives like Midjourney, Stable Diffusion, and Adobe Firefly. The verdict: gpt-image-1-mini is not your showpiece tool, but it is the MVP for bulk content, batch iterations, and last mile edits when “good enough” and “on time” are the goals. We walk through real-world workflows for batch thumbnails, ad variants, product mockups, and storyboard frames. The bottom line: you can test 100 ideas for just a few bucks, hang at a beginner level with simple prompts and masks, or tune things up with more advanced features if you are a pro.

Hear about practical limitations, community experiments, and where mini fits in the new landscape of API-first, budget-forward image models. If you want a reliable, plug-and-play image generator for fast creative projects, mini could be your new favorite tool. Stay tuned for real use cases, honest takes, and fun analogies from the field.]]></description>
    <pubDate>Wed, 08 Oct 2025 19:46:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251008145348464+-+gpt_image_1_mini_daily.mp3" length="3935856" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1759952783766</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251008145348464+-+gpt_image_1_mini_daily.jpg"/>
  </item>

  <item>
    <title>Sora 2 Joins Artlist: All-in-One Video Creation Powerup</title>
    <description><![CDATA[Big news for creators: Sora 2 is now live inside Artlist, meaning instant access to high-fidelity AI video generation without chasing an OpenAI invite. This episode unpacks what makes the Artlist x Sora 2 launch a game changer for YouTubers, marketers, agencies and solo makers. Consolidate video gen, stock, SFX, voiceover, and licensed music under one roof. We break down how Artlist’s credits system works across different plans, and why bundling these tools means faster pitching, fewer legal headaches, and less tool-juggling. Hear which creators and teams will benefit most, plus how it stacks up against other platforms like Google Veo 3 with Flow, Runway, and Adobe’s ecosystem. We get into practical examples: from TikTok product drops to filmmaker pre-visualization and pitch frames—all with commercial licensing covered, no more rights-chasing marathons. Learn the trade-offs, dealbreakers, and who should keep an eye on feature updates. The future of creator-friendly, production-legal AI video is here, and Sora 2 in Artlist is setting a new bar.]]></description>
    <pubDate>Sun, 12 Oct 2025 22:58:44 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251012162929884+-+sora2_artlist_creators.mp3" length="2891184" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1760309924463</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251012162929884+-+sora2_artlist_creators.jpg"/>
  </item>

  <item>
    <title>Grok Imagine Turbocharges AI Video with Multi-Render</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we dive into xAI’s Grok upgrades shaking up short-form video creation. The new Grok Imagine delivers parallel variants from a single prompt, speeding content creation for TikTokers, agencies, and YouTubers. The highly anticipated “Eve” voice gives a natural, human-like narration right in the app, reducing the need for third-party TTS tools. xAI’s push is toward lightning-fast draft-to-publish workflow: create synced six-second clips, batch test promo hooks, and never fuss with exporting audio again. We break down who benefits most from these upgrades, from solo creators to brand teams, and discuss the new “spicy” mode’s built-in moderation filters. Plus, learn how xAI’s “world models” tease could transform indie game development by 2026 and what Macrohard might mean for automated coding. Stay tuned for a full competitive landscape, pricing rundown, and why Grok’s mobile-first speed and built-in voice features set it apart from OpenAI’s Sora, Google’s Veo, and more. Our hosts explain how Grok Imagine’s speed and variety can reduce tool-hopping, increase creative throughput, and change the AI video game. Grab your coffee and join us as we cover the latest in creator-centric AI tools.]]></description>
    <pubDate>Tue, 14 Oct 2025 01:40:48 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251013211613688+-+grok_imagine_multirender.mp3" length="3646392" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1760406048287</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251013211613688+-+grok_imagine_multirender.jpg"/>
  </item>

  <item>
    <title>AI Video for a Nickel: Fal Launches $0.01/s Kandinsky-5</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we dive into Fal.ai’s game-changing move: text-to-video creation with Kandinsky-5 starting at just one cent per second. That means five seconds for five cents—cheaper than your iced coffee! Hosts Zane and Pippa break down what this means for creators, from social teams to indie agencies and educators. With two variants—Distill for rapid, budget-friendly drafts and Standard for higher quality—Fal's offering is all about low-cost, high-iteration workflows at social-friendly resolutions. We compare Fal’s new pricing to competition like Sora 2 on Artlist and Runway, explaining the wild savings for creators who need to test ideas rapidly. Plus, we talk practical use cases: TikTok drafts, YouTube intros, pitch storyboards, animator mood reels, and even on-set previsualization. Learn about the limits, like resolution caps and working within short 5 or 10-second clips, and the transparency offered by AI Forever’s open model docs. Whether you’re a solo hustler or running a team, this episode explores how Fal and Kandinsky-5 make rapid AI video a daily habit instead of a special treat. From “idea confetti” to predicting a new draft aesthetic, we cover why this move just might reshape creative workflows for good.]]></description>
    <pubDate>Tue, 14 Oct 2025 23:05:31 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251014180611746+-+fal_kandinsky5_nickel.mp3" length="4132080" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1760483131893</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251014180611746+-+fal_kandinsky5_nickel.jpg"/>
  </item>

  <item>
    <title>Google Veo 3.1: Native Vertical Video &amp; End-Frame Control</title>
    <description><![CDATA[Today we break down Google Veo 3.1’s game-changing release for video creators in AI Studio and the Gemini API. The headline features: native vertical video (9:16 format) and powerful end-frame control, letting you call your final shot with precision. No more awkward reframes or patchwork edits—vertical is now the master format, streamlining workflows for Shorts, Reels, TikTok, stories, and brand campaigns. We compare Veo 3.1’s editorial upgrades and stability improvements to rivals like Sora 2, Runway, and Luma, highlighting who benefits most: social teams, indie founders, podcasters, designers, and anyone posting short-form video content. We cover how end-frame targeting, scene extension, and character reference images make multi-part stories and series workflows much smoother. If you are tired of wrestling with aspect ratios or losing identities across shots, Veo 3.1 means less manual clean-up and more creative control. Learn practical workflow tips, availability details, pricing guidance, and why this shift from ‘cool AI demo’ toward real production control matters. Discover what creators, brands, and editors can unlock today—and why the era of cropping is over. Pour one out for 2023’s cropped faces: with Veo 3.1, the future is vertical—and it’s in your hands.]]></description>
    <pubDate>Wed, 15 Oct 2025 18:14:46 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251015135017530+-+google_veo_3_vertical.mp3" length="3612528" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1760552086430</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251015135017530+-+google_veo_3_vertical.jpg"/>
  </item>

  <item>
    <title>ChatGPT Atlas: OpenAI Drops the AI Super Browser</title>
    <description><![CDATA[OpenAI just launched ChatGPT Atlas, its first-ever web browser with ChatGPT built right in—no more clunky extensions or hopping between apps. Atlas debuts on macOS, with Windows, iOS, and Android on the way. What makes Atlas different? A page-aware sidebar that understands the content you’re viewing and a new Agent Mode, previewed for Plus, Pro, and Business users, lets you perform supervised multi-step actions like navigating sites, filling forms, and collecting info—all within one tab. Privacy advocates breathe easy: Browser Memories are off by default and never used for model training unless you opt in. Power users, creators, and everyday web surfers all get streamlined workflows, from building podcast scripts to designing mood boards or auto-generating captions. We break down how Atlas compares with Copilot in Edge, Arc Max, and the classic Chrome extensions, explore its impact on productivity, cover the guardrails for Agent Mode, and call out who stands to benefit most. If you’ve ever dreamed of your tabs pulling their own weight, it’s time to meet Atlas. Catch all the news, caveats, and meme-worthy scenarios in today’s Blue Lightning AI Daily recap.]]></description>
    <pubDate>Thu, 23 Oct 2025 01:58:33 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251022214909899+-+chatgpt_atlas_browser.mp3" length="4297296" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1761184713725</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251022214909899+-+chatgpt_atlas_browser.jpg"/>
  </item>

  <item>
    <title>Adobe Firefly Goes Unlimited: Your R&amp;D Sprint Starts Now</title>
    <description><![CDATA[It is a big day for creative workflows! Adobe has just launched unlimited image and video generations in Firefly and Creative Cloud Pro, but only until December 1. This exclusive window lets creators, designers, editors, and studios stretch their imagination without worrying about generative credits or monthly caps. Tap into Adobe’s latest Firefly Image Models, experiment with the upgraded Firefly Video Model, and access integrated partner models including Google’s Imagen and Veo directly in the Firefly web app. Instantly compare multiple looks and aesthetics side by side, then seamlessly transfer assets into Photoshop, Illustrator, Premiere Pro, or After Effects for polished results. Whether you are a solo artist, social media creator, small studio, or agency, this window gives you the chance to prototype, iterate, and stress-test your brand’s visual language at high speed and scale. Key features like Content Credentials ensure provenance on every output, supporting client trust and compliance. There are a few fine-print notes: unlimited generations are web app only, partner model regional access may vary, and after December 1, credits return. But for now, unleash your experiments and build a deep asset library. No extra fees for current Firefly or Creative Cloud Pro subscribers—and no watermark, just receipts for every comp. Listen for practical workflows, competitive comparisons, and why this shift signals a new era of model choice inside creative suites. The clock is ticking: will you use the unlimited moment to level up your content pipeline?]]></description>
    <pubDate>Wed, 29 Oct 2025 02:08:53 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251028215254063+-+adobe_firefly_unlimited.mp3" length="3445032" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1761703733410</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251028215254063+-+adobe_firefly_unlimited.jpg"/>
  </item>

  <item>
    <title>Claude Haiku 4.5: Fast, Cheap, and Creator-Friendly AI</title>
    <description><![CDATA[Happy Halloween and happy Friday! Today’s Blue Lightning AI Daily dives deep into Anthropic’s new lightweight model, Claude Haiku 4.5. This speedy AI delivers reliable, sub-second responses and comes in at just $1 per million input tokens and $5 for outputs. Designed for creators, developers, and studios who need rapid brainstorming, batch content variants, scripts, outlines, and micro-apps without burning a hole in the budget. We break down its impressive SWE-bench Verified score, wallet-friendly cost levers like prompt caching and batch APIs, plus real-world workflow impacts: creators can reclaim real time by skipping AI wait screens and only using bigger models for tough tasks. We also size up the competition in the fast-lane like Google Gemini Flash and OpenAI’s light models and explain how Haiku’s tight ecosystem with Claude.ai, Amazon Bedrock, and Google Cloud Vertex AI makes it easy for anyone from hobbyists to compliance-focused teams. Plus, we give a quick look at Alibaba’s Qwen3-Omni, a new multimodal model for voice, video, and hands-free workflows. If you create, prototype, or iterate on content fast, this episode will clue you in on why you’ll probably want Claude Haiku 4.5 in your workflow stack. Tune in to hear just how much time and money you can save with the speed-first AI revolution.]]></description>
    <pubDate>Sat, 01 Nov 2025 00:33:01 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251031200951904+-+claude_haiku_45_fast_cheaper.mp3" length="3564288" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1761957181313</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251031200951904+-+claude_haiku_45_fast_cheaper.jpg"/>
  </item>

  <item>
    <title>Google Nano Banana Pro: No More Melty Poster Text</title>
    <description><![CDATA[Google DeepMind just launched Nano Banana Pro, its highest-fidelity image generation and editing model yet—and the name is just the start of the fun. In this episode, Zane and Pippa break down why this release matters: reliably readable text in images, deeper creative control over lighting and composition, and easy, model-driven edits. Creators can finally make posters, packaging comps, thumbnails, and banners with crisp, professional typography that does not morph into gibberish. The rollout spans the Gemini app for everyday users and enterprise access via Gemini API, Google AI Studio, and Vertex AI—plus Adobe is bringing Nano Banana Pro into Firefly and Photoshop for direct use in pro workflows. We discuss SynthID watermarking for provenance, tips for strong, creative prompts, and how this model’s improvements can save hours per project. Whether you’re a solo creator or a whole design team, learn how Nano Banana Pro could reshape your daily workflow. We also cover how it stacks up to Midjourney, Ideogram, and Stable Diffusion on text and composition, its limitations like watermarking and small text handling, and pro tips on getting predictable results. Meme moment: “Your poster text… finally readable.”]]></description>
    <pubDate>Thu, 20 Nov 2025 17:02:06 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20251120113031799+-+nano_banana_pro_text.mp3" length="4302216" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1763658126195</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20251120113031799+-+nano_banana_pro_text.jpg"/>
  </item>

  <item>
    <title>Adobe Firefly’s Prompt to Edit Will Save Your Videos</title>
    <description><![CDATA[Ever wished you could zap away a neon shirt photo-bomber or make a background sign disappear without hours of tedious editing? Adobe Firefly Video’s new Prompt to Edit feature is here to grant your most annoying video wishes and it just dropped in public beta. In today’s episode, Hunter and Riley break down how Prompt to Edit lets creators make detailed changes to existing footage using only text instructions. Is it actual magic? Almost: type what you want changed, get clip variations, and iterate—no need for reshoots or suffering through endless hours of masks and roto. We look at what edits this tool is actually built for, like removing objects, swapping backgrounds, changing the mood with lighting tweaks, and even replacing prop colors. It is a massive relief for YouTubers, agencies, TikTokers, and anyone stuck making tiny but critical revisions to video projects. But there are catches. Video editing is still trickier than images, with possible warping or fuzzy results—especially if you love wild handheld shots. Hunter and Riley dish honest advice: start with clear, simple footage and keep prompts specific. Plus, why Firefly’s new multi-model power, including a Runway Gen-4.5 partnership, points to an all-in-one workflow future—so creators can stop juggling tool tabs and actually get paid for creativity, not misery. We also go behind the scenes at CES to meet the opera-singing AI panda, unpack what it means for creators when AI models misrepresent their abilities, and laugh about X’s Community Notes fact-checking literal jokes. The AI world is wild, but Prompt to Edit is raising the bar for making deliverables easier, faster, and way less soul-crushing. Tune in for practical tips and a few hilarious AI meltdowns along the way.]]></description>
    <pubDate>Fri, 09 Jan 2026 17:14:28 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260109110043135+-+firefly_prompt_to_edit.mp3" length="3346632" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1767978868841</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260109110043135+-+firefly_prompt_to_edit.jpg"/>
  </item>

  <item>
    <title>LTX-2: Open AI Video Meets Sync Audio Game-Changer</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we dig into Lightricks’ headline drop: the open-source release of LTX-2. Why is everyone buzzing? LTX-2 does what AI video has rarely managed—generates video with native, time-synced audio. That means drafts look and sound closer to the real thing, with music, ambience, dialogue, and effects synced right to the visuals. Hunter and Riley break down why this is a massive upgrade for creators and teams, replacing silent previews and janky temp tracks with instant rough cuts that feel alive. Even better, LTX-2 is open weights and local-first, so agencies and brands can control privacy, cost, and workflow from day one—no cloud uploads required. But with power comes new challenges, from technical setup (hello GPU troubleshooting) to style drift, IP management, and the need for an actual AI style guide. Audio quality is impressive but not flawless, especially in complex scenes. The pair explore what creators can expect, how LTX-2 shifts the bottleneck in production, and why client expectations are about to get even trickier. Also, in a quickfire CES round-up: hologram buddies, AI lollipops, sassy fridges, dancing robot dogs, and smart LEGO—proving that sometimes the least weird AI news is the most useful. If you care about streamlined creative workflows, privacy, and moving beyond “fixing it in post,” this episode is for you.]]></description>
    <pubDate>Sat, 10 Jan 2026 17:50:32 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260110122249942+-+ltx2_video_audio_sync.mp3" length="3483264" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768067432669</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260110122249942+-+ltx2_video_audio_sync.jpg"/>
  </item>

  <item>
    <title>AI Wants Your Decks: PowerPoint Autocomplete Arrives</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, Hunter and Riley break down OpenAI's bold next phase: asking contractors to upload real workplace deliverables like PowerPoint decks, spreadsheets, and code repos. Why does this matter? It's not just about more data; it's about better data. Training AI on actual business artifacts could mean generative agents that deliver finished, client-ready work in native file formats, not just essays or blog posts. But with opportunity comes risk: can humans really sanitize sensitive corporate data before uploading? Is removing a logo enough when the real secrets are in the structure and presentation? The hosts explore the line between useful AI assistants and dangerous corporate oversharing, plus shoutouts to “Superstar Scrubbing,” a privacy tool that's as fun to say as it is controversial to trust. Meanwhile, new releases like Lightricks’ LTX-2 and Adobe Firefly hint at an industry shift from simple generation to true completion, where AI delivers outputs that actually play, edit, and present. Falcon H1R-7B’s long context capabilities show why this change matters for creators whose real work lives in massive, messy stacks of brand guidelines and feedback. The episode closes with practical tips for creators and marketers: structure your briefs, safeguard your data, and keep humans in the loop. All this and more, with far fewer vibe checks than your average spreadsheet.]]></description>
    <pubDate>Sun, 11 Jan 2026 17:47:29 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260111122147825+-+ai_wants_your_decks.mp3" length="2822208" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768153649558</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260111122147825+-+ai_wants_your_decks.jpg"/>
  </item>

  <item>
    <title>Adobe Firefly Adds GPT Image 1.5: Smarter Text, Smoother Workflow</title>
    <description><![CDATA[Today on Blue Lightning Daily, Hunter and Riley unravel Adobe Firefly’s bold move to integrate OpenAI’s GPT Image 1.5 as a partner model. This update means you can now tap into OpenAI’s powerful image generation, right inside Firefly—no more bouncing between apps or wrestling with file chaos like 'final_final_FORREAL.psd.'

We break down how this shift transforms Firefly into a true creative cockpit, letting you pick the best AI engine for each stage of a project. If you’ve ever groaned at AI-generated text that looked like it was created by a haunted typewriter, brace yourself: GPT Image 1.5 promises legible, prompt-following, brand-ready typography. This episode is packed with practical tips for creators and marketing teams, from nailing those approval-ready comps to knowing when to reach for Adobe’s native model versus OpenAI’s for pixel-perfect results.

We get real about model literacy, discuss how workflows are evolving towards fewer handoffs and more reliable drafts, and debate whether AI finally spelling things right will end the era of funny, cursed text memes. Plus: a look at Adobe’s strategic play to own the creative workflow, creator challenges for this week, and why getting from brief to deliverable has never been smoother—or funnier.

Tune in for tips, snark, and the latest in AI tools that make Monday easier—not just more impressive.]]></description>
    <pubDate>Tue, 13 Jan 2026 18:37:45 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260113122337463+-+adobe_firefly_gpt_image.mp3" length="3356400" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768329465643</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260113122337463+-+adobe_firefly_gpt_image.jpg"/>
  </item>

  <item>
    <title>Google Veo 3.1: Native Vertical AI Video Arrives</title>
    <description><![CDATA[Goodbye to awkward crops and hello to native vertical AI video! In today’s Blue Lightning Daily, Hunter and Riley break down the arrival of Google Veo 3.1, the upgrade that finally treats vertical video like a first-class citizen. The hosts explain why true 9 by 16 output is more than a new aspect ratio: it is the difference between endless post-production tweaks and actually hitting shipping deadlines. They walk through Veo 3.1’s new features, including upscaling, reference image support, first and last frame controls for pro editing, and stronger consistency. Find out why cropping is chaos for marketing teams, how AI’s “cousin swap” phenomenon hurts brand storytelling, and why controls for art direction matter for both agencies and solo creators. Plus, hear how watermarking and provenance fit into AI video’s future, and why every workflow needs to plan for those odd AI-generated hands. If your team creates social video at scale, you will not want to miss this episode. Learn what Veo 3.1 really means for creativity, automation, and your next viral ad.]]></description>
    <pubDate>Wed, 14 Jan 2026 18:00:18 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260114122346058+-+google_veo_31_vertical.mp3" length="4207944" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768413618289</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260114122346058+-+google_veo_31_vertical.jpg"/>
  </item>

  <item>
    <title>Adobe Firefly: From Clip Generator to Real Video Editor</title>
    <description><![CDATA[Is Adobe Firefly’s video editor beta quietly morphing into a serious, browser-based editing hub? Today, Riley and Hunter dive into Firefly’s new multi-track timeline, transcript-based editing, and integrated Generation History. No more exporting sad demo clips and bouncing files between downloads and mysterious folders. Now, creators can actually assemble, review, and iterate—all in-browser. The hosts discuss why transcript editing feels game-changing for rough cuts (but not always for legal review), how multi-track timelines turn Firefly from a toy into a workflow tool, and how Generation History saves your best takes from chaos. Plus, they debate the rise of template culture—is this the Canva-fication of video—and why Adobe’s core play seems less about winning the model race and more about controlling the workspace where creators get work done. You’ll also hear how these new tools supercharge fast review cycles and compress the dreaded early edit loop, but why a solid internet connection—and a final human pass—are still essential. If you’ve ever lost a file named final_final_FORREAL, this one’s for you. Tune in for a fun, practical take on Firefly’s evolution and the direction of browser-based creative AI.]]></description>
    <pubDate>Thu, 15 Jan 2026 18:27:53 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260115122639007+-+adobe_firefly_video_ep.mp3" length="3942192" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768501673046</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260115122639007+-+adobe_firefly_video_ep.jpg"/>
  </item>

  <item>
    <title>GLM-Image: The AI That Typesets Without Tears</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, creators and marketers finally get the feature they've been begging for: true marketing-grade AI-generated visuals with readable, usable text. Zhipu AI has open-sourced GLM-Image—a major leap in image generation models that can handle actual poster headlines, sharp product shots, and those tricky little disclaimers without turning your copy into a cryptic alphabet soup. We dive into why typography is so tough for AI, how GLM-Image claims to solve it, and why open weights matter for creative teams needing control, repeatability, and privacy. Plus, we walk through the first-hour sniff test every creator should use: can GLM-Image nail a YouTube thumbnail, promo ad, and full event poster with all the details? Hear why the model architecture lets it plan layouts and render clean text, what the open-source angle really means for workflows, and where brand consistency is still the ultimate challenge. Along the way, we break down how the AI ecosystem is racing from fun demos to real production tools—across Sora 2’s video and Adobe’s workflow moves. And, as always, a look at the chaos and charisma of modern AI: delivery robots versus trains, trendy TikTok dramas, and why every new model should be tested like an overconfident intern. Get hands-on advice, spicy takes, and a practical playbook for running GLM-Image through its paces. Join us for a fast, fun hit of what matters in generative AI visuals right now.]]></description>
    <pubDate>Sun, 18 Jan 2026 18:25:19 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260118122413034+-+glm_image_typography.mp3" length="3804072" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768760719265</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260118122413034+-+glm_image_typography.jpg"/>
  </item>

  <item>
    <title>ChatGPT Sponsored Messages and the New Go Tier Explained</title>
    <description><![CDATA[OpenAI is transforming ChatGPT from a simple assistant into a fully-fledged discovery platform—and things just got even more interesting. This episode dives into the debut of sponsored messages inside ChatGPT and the announcement of ChatGPT Go, an $8 monthly plan that offers more usage and features, but still includes ads. Hosts Hunter and Riley unpack why this middle tier is getting attention, and how the line between user assistance and advertising is becoming blurrier. How do sponsored messages really shape user behavior, and what does this mean for trust in the AI you talk to every day? Creators get unique insights on adapting their products for the new era of ChatGPT-centric workflows: why templates, custom GPTs, and plug-and-play prompt packs are becoming the new gold standard, and how to position yourself for organic discovery in an AI platform that’s suddenly full of ads. Plus, a lightning roundup of other AI ecosystem updates: Sora 2, PixVerse R1, Adobe Firefly’s web video editor, and GLM-Image’s open-source leap. If you’re building for, selling in, or just curious about where the AI internet is headed, this episode will get you up to speed—and arm you with a few strong opinions on the future of sponsored discovery.]]></description>
    <pubDate>Mon, 19 Jan 2026 21:30:30 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260119122129927+-+chatgpt_go_sponsored.mp3" length="3379704" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768858230907</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260119122129927+-+chatgpt_go_sponsored.jpg"/>
  </item>

  <item>
    <title>Sora 2: AI Video Gets a Social Remix</title>
    <description><![CDATA[OpenAI just dropped Sora 2, and it's not just about better text-to-video AI. Imagine TikTok, but every clip is generated and remixable at lightning speed. In this episode, Hunter and Riley explore how Sora 2 packages creation, discovery, and audience into one addictive app. Swipeable feeds, instant remixing, and identity 'Cameos' let you cast yourself or friends directly into scenes. What does this mean for creators? Less time spent writing prompts, more time shipping variations. It is fast, fun, and a little brain-melting. But who actually owns the audience—creators, or OpenAI’s algorithmic feed? And what happens to originality when remixing is the default? Plus, the team unpacks the new risks: deepfakes, trend sameness, and the creative trap of letting the feed set all your ideas. Cameos make identity casting easy, but require strong guardrails for brands and transparency around 'synthetic authenticity.' The story connects with the broader AI video landscape too, including PixVerse’s new real-time world mode and GLM-Image’s open-source boost in design skills. Bottom line: Sora 2 is workflow-native and all about speed, but humans still need to guide strategy, review, and keep things fresh. Tune in for a creator-first perspective on the shifting balance between AI automation and human taste–and how to keep your content from turning into soup.]]></description>
    <pubDate>Tue, 20 Jan 2026 22:01:35 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260120122556999+-+sora_2_ai_goes_social.mp3" length="3251256" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1768946495726</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260120122556999+-+sora_2_ai_goes_social.jpg"/>
  </item>

  <item>
    <title>Adobe Premiere Pro 26: Object Mask and the End of Rotoscoping?</title>
    <description><![CDATA[Adobe just dropped Premiere Pro 26 and the star is Object Mask, a new AI-powered tool promising to make rotoscoping a thing of the past. In this episode, Hunter and Riley break down exactly how Object Mask can save editors from frame-by-frame drudgery by tracking people or objects with a click. We unpack the difference between local on-device AI masking and cloud-dependent Firefly generation, plus the real-world masking pain points like hair, motion blur, and those moments where the mask picks Dua Lipa's face from the background instead of your subject. The episode also tackles workflow hacks for combining Object Mask with new shape tools, how to survive the flood of client micro-edits, and why the tighter Frame.io panel could prevent version-control madness. We compare the “start with footage” approach of Premiere against the wild generative discovery sweeping tools like Sora 2, and discuss what Adobe needs to get right so creators do not jump ship for faster or cheaper alternatives. If you are a volume editor working on social, UGC, or marketing content, this is the time to try Object Mask on your real-world edits. Find out why the most valuable upgrade might be not just speed, but consistency and sanity for teams of every size.]]></description>
    <pubDate>Wed, 21 Jan 2026 22:09:05 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260121122228719+-+premiere_26_object_mask.mp3" length="3796344" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769033345700</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260121122228719+-+premiere_26_object_mask.jpg"/>
  </item>

  <item>
    <title>Adobe Firefly: AI Fixes Timelines and Fakes Reality</title>
    <description><![CDATA[Today on Blue Lightning Daily, we dive deep into Adobe's latest push to make AI an essential editing tool right inside Premiere Pro and After Effects. Discover why the new Generative Extend feature is less about sci-fi magic and more about saving editors from tedious micro-edits. Is this the end of endless padding and awkward transitions or the dawn of infinite client revisions? We debate whether AI-powered edits could quietly alter meaning and what creators and brands should watch out for in the age of time-bending tools. Plus, learn how Object Mask aims to turn the pain of subject isolation into a one-click breeze, and how After Effects upgrades like native 3D parametric shapes and SVG import are boosting designers. We cover real workflow wins, new ethical puzzles, the impact for solo creators versus teams, and what you should know about cloud versus local AI. If you care about video, marketing, or creative tech, this episode distills what matters most, what to try, and what to watch out for. Tune in for practical advice, internet memes, and the AI-assisted editing revolution in real time.]]></description>
    <pubDate>Thu, 22 Jan 2026 18:25:58 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260122122802530+-+adobe_firefly_timeline.mp3" length="3725256" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769106358757</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260122122802530+-+adobe_firefly_timeline.jpg"/>
  </item>

  <item>
    <title>Adobe Firefly Foundry: AI Goes Corporate</title>
    <description><![CDATA[Adobe just launched Firefly Foundry and it is changing how generative AI works inside big creative teams. Instead of just making cool images, Foundry gives brands and agencies a way to create custom AI models trained on their own approved assets. That means every creative, campaign, or client can have their own model with guardrails on brand, IP, and security. This episode breaks down why Foundry matters for anyone struggling with brand guidelines, endless edits, and legal reviews. We talk about why paperwork is now a feature, how governance and audit trails make AI production-safe, and why Creative Cloud integration is key for adoption. We also tackle open questions: Can these models fix office politics and subjective feedback? Will enterprises finally use AI for real projects? Is the “on-brand” dream even possible if nobody agrees what on-brand means? Plus, we look at what this means for agencies juggling multiple clients, and for creators who need to balance speed with risk. We also touch on the week’s other big AI news: OpenAI testing sponsored messages in ChatGPT and what happens when your brainstorming chat starts serving you ads. Tune in for the most relatable, funny, and practical take on where AI for teams is heading next—and how to avoid disasters with smarter pipelines.]]></description>
    <pubDate>Fri, 23 Jan 2026 17:50:24 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260123122443763+-+adobe_firefly_foundry.mp3" length="3515544" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769190624772</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260123122443763+-+adobe_firefly_foundry.jpg"/>
  </item>

  <item>
    <title>OpenAI Atlas: Tab Groups and Auto Mode Save Your Sanity</title>
    <description><![CDATA[Join Hunter and Riley for a deep dive into OpenAI’s latest ChatGPT Atlas update for macOS. It’s not about smarter AI models, but about making your research workflow a lot more manageable. Atlas now introduces Tab Groups, an Auto search mode that chooses between ChatGPT responses and Google results, and a revamped results layout—all designed to help creators keep research organized, context-rich, and fast-paced. The hosts break down how Atlas tackles tab overload, what smart tab grouping can do for your creative or editorial process, and why Auto mode’s helpful intern energy is a boost (with some caveats). Expect smart tips for managing creative chaos, a look at how Adobe is making Firefly Foundry part of the everyday workflow, and a discussion on how AI tools are becoming the backbone of real production. Whether you routinely hoard tabs or need strategies to keep your sources and inspiration separate, this episode is full of hands-on advice, thoughtful warnings about treating AI as your only source, and a peek at the shifting role of the browser in modern content creation. It’s the episode every research-inclined multitasker needs.]]></description>
    <pubDate>Sat, 24 Jan 2026 17:57:19 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260124122129389+-+atlas_tab_groups_auto.mp3" length="2996184" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769277439189</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260124122129389+-+atlas_tab_groups_auto.jpg"/>
  </item>

  <item>
    <title>Runway Gen-4.5 Makes AI Video Consistent</title>
    <description><![CDATA[Today on Blue Lightning Daily, Hunter and Riley dive into Runway’s Gen-4.5 and what it really means for creators. Forget trippy demos and melting faces. This new launch promises actual consistency in image-to-video, so creators can finally count on stable characters, steady backgrounds, and less “tweak detonation” when making adjustments. No more shape-shifting wardrobes and elastic hands. Gen-4.5 is not about making full movies but giving you reliable shots that you can actually use and stitch together. 

The other game-changer? Gen-4.5 is now integrated with Adobe Firefly. That means creators and whole teams can generate, concept, and edit within their familiar Adobe workflow, moving seamless from stills to full edits in Premiere Pro and After Effects, cutting down chaos and file confusion.

But the pros and cons are real. The Adobe Firefly rollout is capped at 720p and 10-second clips so you cannot go wild with feature-length films yet. Still, these boundaries help focus on what AI video does best right now: short, editable shots that speed up creative workflows. The real challenge becomes picking the best clips and making creative decisions, not just asking the AI for “one more version.”

Plus, Hunter and Riley talk about the wider push for workflow integration across platforms, the importance of keeping AI outputs verifiable, and why safety and context labeling are a must as automation settles into creative pipelines. From practical wins to the cautionary chaos of over-trusting AI, this episode covers the state of real-world video creation in 2026.]]></description>
    <pubDate>Sun, 25 Jan 2026 18:34:30 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260125122403639+-+runway_gen45_firefly.mp3" length="3514104" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769366070267</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260125122403639+-+runway_gen45_firefly.jpg"/>
  </item>

  <item>
    <title>NVIDIA PersonaPlex: The End of Awkward AI Conversations</title>
    <description><![CDATA[Say goodbye to robotic, walkie-talkie-style AI voices. On today’s episode, we dive into the open release of NVIDIA PersonaPlex-7B-v1, a speech-to-speech model that can actually hold a real back-and-forth conversation. Built for full-duplex communication, PersonaPlex listens and talks at the same time, handles interruptions, and brings natural rhythm to AI-driven dialogue. We break down why this model is a game-changer for creators, live hosts, and anyone building audio products. Discover how PersonaPlex moves beyond clunky handoffs between old-school speech-to-text and large language models, and nearly erases awkward pauses. Hear tips on setting up creative “producer goblin” AI co-hosts, why backchanneling is both a superpower and a potential dark pattern, and how brands can avoid the “smug millennial barista” voice trap. We also spotlight creator-friendly use cases, from improv livestreams to real-time chat recaps, and discuss what open weights mean for teams of all sizes. Plus, stay tuned for a rapid-fire roundup on Runway Gen-4.5, Adobe Firefly’s latest integrations, and OpenAI ChatGPT Atlas’s new tab-wrangling tricks. If you want to know where conversational AI and creator tools are headed, this episode puts you right at the center of the action.]]></description>
    <pubDate>Mon, 26 Jan 2026 18:13:54 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260126122430927+-+nvidia_personaplex_ai.mp3" length="2979504" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769451234059</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260126122430927+-+nvidia_personaplex_ai.jpg"/>
  </item>

  <item>
    <title>OpenAI Prism: AI Moves Into Your LaTeX Docs</title>
    <description><![CDATA[OpenAI just launched Prism, a free browser-based writing workspace designed for creators and researchers who live in LaTeX and crave organization. Prism seamlessly integrates GPT 5.2 right inside your doc, tackling citations, equations, and document structure without juggling apps. It promises to automate all those technical headaches like citation flows and consistency checks, even turning your sketches into editable diagrams with its image-to-diagram feature. Real-time coauthoring means your whole team can jump in, making Prism a giant leap for technical collaboration and productivity. But is it all upside? Hunter and Riley break down how Prism rewrites the rules for structured writing and explain why human editorial skills still matter. From instant notation cleanup to voice-powered revisions, you’ll hear exactly who should check out Prism—and who should be cautious about workflow lock-in or over-trusting the model. Plus, the hosts unpack why AI moving into your workspace is way more than just a new sidebar, and why keeping your own critical eye is still essential. Perfect for technical marketers, product teams, and anyone who needs to get complex ideas across (with proof).]]></description>
    <pubDate>Wed, 28 Jan 2026 17:44:51 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20260128122453779+-+openai_prism_latex.mp3" length="3465696" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1769622291276</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20260128122453779+-+openai_prism_latex.jpg"/>
  </item>
</channel>
</rss>
