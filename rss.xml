<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
     xmlns:atom="http://www.w3.org/2005/Atom">

  <channel>
    <title>Blue Lightning AI Daily</title>
    <link>https://yourdomain.com/podcast</link>
    <language>en-us</language>
    <copyright>© 2025 Blue Lightning</copyright>
    <lastBuildDate>Tue, 23 Sep 2025 13:55:54 GMT</lastBuildDate>
    <pubDate>Sun, 31 Aug 2025 12:00:00 GMT</pubDate>
     <itunes:category text="Business">
       <itunes:category text="Entrepreneurship"/>
     </itunes:category>
     <itunes:category text="Technology">
       <itunes:category text="Tech News"/>
     </itunes:category>
    <generator>RSS Builder</generator>
    <description>
      Blue Lightning AI Daily is your go-to AI podcast for creators, delivering fast, focused updates on the world of generative AI. We cover the latest breakthroughs in large language models (LLMs), AI video editing, AI photography, AI audio tools, and creative automation. Each episode gives digital creators, content marketers, and creative professionals clear insights into how AI is transforming storytelling, production, and the creator economy. Stay informed, stay creative, and stay ahead with daily AI news made simple.
    </description>

    <!-- iTunes / Apple Podcasts tags -->
    <itunes:author>Blue Lightning</itunes:author>
    <itunes:summary>
      Blue Lightning AI Daily is your go-to AI podcast for creators, delivering fast, focused updates on the world of generative AI. 
      We cover the latest breakthroughs in LLMs, AI video, AI audio, and creative tools.
    </itunes:summary>
    <itunes:explicit>no</itunes:explicit>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/blue-lighting-cover.jpg"/>
    <itunes:owner>
      <itunes:name>Blue Lightning</itunes:name>
      <itunes:email>ted@coey.com</itunes:email>
    </itunes:owner>

    <!-- Atom self-link (recommended for apps like Apple Podcasts / Overcast) -->
    <atom:link href="https://raw.githubusercontent.com/ted-coey/bluelightingpodcast/main/rss.xml" rel="self" type="application/rss+xml"/>

    <!-- Episodes will be inserted here as <item> blocks -->

  <item>
    <title>Nano Banana: The Meme-Powered Image Revolution</title>
    <description><![CDATA[Discover why creators are obsessed with “Nano Banana”—Google’s new high-consistency image tool in Gemini. Learn how stable likeness, fast multi-step edits, and watermark safeguards are changing workflows for artists, marketers, and everyday users. We break down costs, real-world uses, and pro tips for perfect prompts, plus why this fun meme-model is winning fans across the AI art community.]]></description>
    <pubDate>Sun, 31 Aug 2025 19:38:18 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/nano_banana_revolution.mp3" length="3761760" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756669098912</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+nano_banana_revolution.jpg"/>
  </item>

  <item>
    <title>Grok Code Fast 1: Blazing Agent Loops, Zero Lag</title>
    <description><![CDATA[We break down xAI’s Grok Code Fast 1, a lightning-speed coding model made for agents that build, test, and fix in sub-seconds. From creators to dev teams, learn how this low-latency engine transforms workflows, slashes context-switching, and enables affordable micro-automation. Find out where to try it now, tips for better results, and why speed matters for creative pros.]]></description>
    <pubDate>Mon, 01 Sep 2025 15:49:19 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/grok_code_fast_1_zero_lag.mp3" length="3825504" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756741759524</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+grok_code_fast_1_zero_lag.jpg"/>
  </item>

  <item>
    <title>Google Vids Superpowers: AI B-Roll, Avatars, and Speed</title>
    <description><![CDATA[Google Vids just leveled up! We break down the new AI features: fast image-to-video b-roll, on-brand AI presenters, and transcript magic, all inside Google Workspace. Dive into how creators, marketers, and teams can storyboard, auto-edit, and publish videos quicker—no pro skills required. Hear workflow shortcuts, big-picture impacts, and why Google is pushing video-first collaboration. Get ready to power up your content game!]]></description>
    <pubDate>Tue, 02 Sep 2025 18:28:17 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/google_vids_ai_boost.mp3" length="3573000" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756837697227</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+google_vids_ai_boost.jpg"/>
  </item>

  <item>
    <title>Go Bananas for On-Model AI Art in Descript!</title>
    <description><![CDATA[Descript’s new “nano banana” image model keeps your characters and products looking consistent across edits. Learn why creators, marketers, YouTubers, and brand designers are obsessed—plus how Google’s watermarking ups trust and safety. From faster thumbnails to brand-safe ads, Blue Lightning AI Daily unpacks this delicious update and introduces the consistency wave rolling through creative AI tools.]]></description>
    <pubDate>Wed, 03 Sep 2025 16:29:11 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/bananas_for_consistency.mp3" length="3552648" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1756916951036</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+bananas_for_consistency.jpg"/>
  </item>


  <item>
    <title>Google Photos Create Tab: Motion Magic Unleashed</title>
    <description><![CDATA[Discover Google Photos’ new Create tab and the Veo 3-powered Photo-to-Video upgrade. We break down fresh Remix styles, cinematic animations, and highlight videos—all in-app. Find out how creators can batch slick stories faster with AI, tap into highlights from old archives, and skip the extra editing hassle. Pro tips included for eye-catching, scroll-stopping visuals. Turn memories into motion—no excuses!]]></description>
    <pubDate>Fri, 05 Sep 2025 22:37:26 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/google_photos_magic.mp3" length="3201240" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757111846421</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Podcast+Image+-+google_photos_magic.jpg"/>
  </item>


  <item>
    <title>Alibaba Qwen3‑Max‑Preview: Trillion-Parameter AI for Creators</title>
    <description><![CDATA[On this episode of Blue Lightning AI Daily, Zane and Pippa dive into Alibaba’s new Qwen3‑Max‑Preview, a trillion-plus parameter large language model making waves for content creators. Is it just big model hype, or does scale translate to workflow wins? Discover how Qwen3‑Max‑Preview offers improved instruction following, long multi-turn chats, and powerful agentic features for planning and tool use. Learn how creators from daily YouTubers and podcasters to filmmakers and brand agencies can benefit from longer context windows, more reliable outputs, and streamlined content pipelines. We break down Alibaba’s pitch versus competitors, pricing insights, and where this model fits in the arms race with OpenAI, Google, and Anthropic. Plus, hear hands-on scenarios for creative pros and hobbyists, pricing comparisons, and warnings about preview limitations. Whether you run a pro studio or create on a shoestring budget, discover how Qwen3‑Max‑Preview changes the game for AI-powered content. Tune in for everything you need to know about this bold new entry in the 2025 AI landscape.]]></description>
    <pubDate>Sat, 06 Sep 2025 17:13:54 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250906130359338+-+qwen3_max_preview_ai.mp3" length="4877424" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757178834891</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250906130359338+-+qwen3_max_preview_ai.jpg"/>
  </item>

  <item>
    <title>Vidu Q1 Drops Seven-Image Continuity for AI Video</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, we break down ShengShu Technology’s latest Vidu Q1 update: multi-reference video generation. Imagine keeping your characters, props, and backgrounds consistent across a whole sequence—not just a shot—using up to seven reference images. Vidu Q1’s Reference-to-Video workflow aims to crush the classic problems like disappearing hats and morphing hero products, letting creators lock in continuity and reduce time lost to patchwork fixes. We compare it with rivals like Runway, Pika, and Luma and explain how more references mean fewer weird wardrobe jumps or identity drift. Find out why this helps everyone from YouTube regulars to ad agencies, plus solo filmmakers tired of reshoots and cleanup. We cover new features like 'First-to-Last Frame' seamless transitions and API-powered automation for brands. Hear practical workflows for character scenes and branded product shots, plus vital tips for prepping references and matching lighting. We also flag limitations like over-rigid faces and ambiguous input images, explain pricing and availability, and dig into how the update fits the evolving battle for AI video control. Will this replace your whole toolkit? Not yet. But for continuity, Vidu Q1’s seven-ref system might be your secret weapon. Tune in for the meme send-off and practical advice you can use today.]]></description>
    <pubDate>Mon, 08 Sep 2025 21:32:32 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250908171755570+-+vidu_q1_seven_refs_update.mp3" length="3828984" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757367152301</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250908171755570+-+vidu_q1_seven_refs_update.jpg"/>
  </item>

  <item>
    <title>Apple vs Pixel: AI-Powered Phones Go Native</title>
    <description><![CDATA[Today on Blue Lightning AI Daily, Zane and Pippa break down the seismic shift in mobile AI as Apple and Google roll out radical new features. Apple’s iPhone event showcased Apple Intelligence—a generative AI layer baked right into your device, with privacy at its core and slick workflow upgrades like Writing Tools and smarter Siri. Meanwhile, Google’s Pixel 9 drops Gemini everywhere: from voice summaries to Docs to fast image gen, all woven across Android and the cloud. We compare Apple’s privacy-first on-device AI with Google’s “AI mesh” that syncs across your digital life. Whether you’re a YouTuber, podcaster, designer, or writer, we dig into real creator use cases—like instant script rewrites, visual thumbnails, clean transcriptions, fast collaboration, and obsessive privacy controls. Which approach saves more time? What about battery life, data safety, and regional rollouts? Plus, we unpack what these moves mean for the whole app ecosystem, pro tools, and that growing subscription subtext around AI. Are you Team Continuity or Team Cloud Mesh? Tune in for the ultimate iPhone vs Pixel AI vibe check—because wherever you land, creators finally win as AI becomes the default layer, not just another app.]]></description>
    <pubDate>Tue, 09 Sep 2025 22:44:54 GMT</pubDate>
    <enclosure url="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/20250909182003491+-+apple_vs_pixel_ai.mp3" length="4323672" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757457894366</guid>
    <itunes:image href="https://blue-lighting-podcast.s3.us-east-2.amazonaws.com/Image+20250909182003491+-+apple_vs_pixel_ai.jpg"/>
  </item>

  <item>
    <title>Nvidia Rubin CPX: AI That Remembers Your Entire Movie</title>
    <description><![CDATA[Ready for AI that never forgets—and keeps your edits in sync? Today we unpack Nvidia’s new Rubin CPX inference chip, built just for creative pros who need their AI to recall every beat of a scene or line of code. Rubin CPX is all about massive context windows: million-token, hour-scale workflows where generative tools never lose the thread.

Zane and Pippa run through the hardware hype: 30 PFLOPs of power, 128 GB of rapid memory, and on-chip video encode and decode. The magic? Rubin CPX fuses media engines with transformer inference right on silicon, so long-form audio, video, dubbing, and code analysis all stay coherent—no more forgetting that intro gag or missing a key plot twist.

Hear why data centers and creative tools are racing to adopt it, and what that means for YouTubers, podcasters, TikTokers, and filmmakers itching for continuous AI-powered workflows. Learn how “context lock” and “style sync” might finally work across your full project, not just short clips. Plus: We break down Nvidia’s bold revenue claims, rollout timeline, and how new workflows could replace tedious chunking and stitching for anyone creating hour-plus content.

Get the scoop on specs, architecture, competitive landscape, and the practical impact for creators and studios. If you’re dreaming of AI that holds the whole story together, Rubin CPX is the tech to watch.]]></description>
    <pubDate>Thu, 11 Sep 2025 15:18:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250911093026288+-+nvidia_rubin_cpx_pod.mp3" length="5063496" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757603903251</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250911093026288+-+nvidia_rubin_cpx_pod.jpg"/>
  </item>

  <item>
    <title>Synamedia’s All-In AI: Smarter Streams and Highlights</title>
    <description><![CDATA[Today we break down Synamedia’s headline-grabbing AI rollout at IBC2025, which brings artificial intelligence to every corner of the streaming and broadcast workflow. From conversational search that lets you instantly find moments like "Messi’s third goal" to live tagging that auto-flags game highlights as they happen, this is a serious upgrade for anyone working with live or on demand video. Synamedia bakes in dynamic creative personalization, contextual ad intelligence, and even an AI video quality agent that tunes encoders on the fly—saving time, money, and network headaches. Quortex Switch debuts in Europe, promising standards-based multi-CDN switching with smarter, real-time routing decisions, while ContentArmor watermarking keeps content secure. We cover who benefits most (think broadcasters, sports streamers, rights holders, and pro creator teams), and why even small creators will feel the ripple effect as platforms get these upgrades. Plus, insights into the industry move toward open standards like OpenMOQ and Media over QUIC for better real-time streaming. If you care about clips, live highlights, quality, or smarter delivery, this episode unpacks the future of broadcast AI—beyond demos, these are workflow game-changers rolling out now.]]></description>
    <pubDate>Thu, 11 Sep 2025 18:05:23 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250911123526354+-+synamedia_ai_stack.mp3" length="4149456" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757613923235</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250911123526354+-+synamedia_ai_stack.jpg"/>
  </item>

  <item>
    <title>Seedream 4.0: 4K Image AI That Means Business</title>
    <description><![CDATA[Ready for true 4K AI images in any dimension and workflow? Today Zane and Pippa break down Seedream 4.0 from ByteDance, the latest contender in the AI image generator world. Seedream 4.0 brings genuine 4K output, custom aspect ratios, and an API built for creators and teams who care about precision, speed, and scalable workflows. We dig into why the exact-pixel control is a game changer for YouTubers, brands, podcasters, and design studios that need reliable asset generation without pixel drama. With claims of being over ten times faster than the last version, plus cost-effective direct pricing at just $0.03 per image, Seedream is positioning itself as perfect for large campaigns and creators who need consistent, production-ready batches. We talk about hands and faces (yep, improved!), lighting coherence, and multi-image orchestrations that save precious hours of cleanup and cropping. We also size up Seedream 4.0 against rivals like Nano Banana, Midjourney, and Ideogram—unpacking ELO benchmarks, versatility, and why this tool feels less like a toy and more like a production engine. Pippa and Zane run through rapid-fire use cases, workflow wins, and practical money-saving tips. Whether you are a solo creative, content team, or developer wiring into Seedream’s batch API, this episode has the need-to-know on today’s fastest and sharpest generative art engine. Is it a must-have or just nice-to-have? Find out, plus learn where and how to get started.]]></description>
    <pubDate>Sat, 13 Sep 2025 15:33:03 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250913110529795+-+seedream_4_0_4k.mp3" length="3578040" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1757777583063</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250913110529795+-+seedream_4_0_4k.jpg"/>
  </item>

  <item>
    <title>YouTube Drops Veo 3 Fast and Speech to Song for Shorts</title>
    <description><![CDATA[YouTube is supercharging content creation with a trio of new AI features dropping right into Shorts: Veo 3 Fast for rapid prompt-to-video generation, Edit with AI for instant draft edits, and Speech to Song that morphs lines of dialogue into catchy musical hooks. On this episode, we break down how Veo 3 Fast lets you spin up 480p clips with audio in seconds, right from the Shorts camera or YouTube Create app. Edit with AI scans your raw footage, drafts a highlight reel, sequences edits, adds music, and even provides voice narration, all in a tap or two. We also dive into Speech to Song, powered by DeepMind’s Lyria 2, which lets creators turn spoken lines into shareable audio hooks complete with auto-attribution. Find out which creators benefit most, how YouTube’s integrated approach compares to TikTok and third-party tools like CapCut or Runway, and what it means for your posting workflow. Plus, updates on YouTube’s policies for AI content, watermarks, and deepfake protection, as well as rollout timing, limitations, and where these features are available first. Discover how these tools help brands, solo creators, podcasters, and even filmmakers rethink their process—saving hours and unlocking new ways to experiment and share ideas instantly on Shorts.]]></description>
    <pubDate>Tue, 16 Sep 2025 18:24:58 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250916141337713+-+youtube_veo_3_fast_ai.mp3" length="3402168" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758047098824</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250916141337713+-+youtube_veo_3_fast_ai.jpg"/>
  </item>

  <item>
    <title>ElevenLabs Studio 3.0: A Timeline Revolution for Creators</title>
    <description><![CDATA[ElevenLabs Studio 3.0 is here and it is shaking up the game for podcasters, YouTubers, audiobook creators, and agencies. In this episode, we dive deep into everything new: true video support, seamless voiceovers and music beds, automatic multilingual captions, and the headline feature, Speech Correction—just edit your text and your own cloned voice matches right up, no more closet re-records. Now, creators can upload video, mix in narration, underscoring, and sound effects, all on a single browser-based timeline. It is designed to cut out app-hopping and let your creative process flow from scriptwriting to finished media in one place. We compare Studio 3.0 to Descript, CapCut, and Adobe, and unpack why ElevenLabs stands out with its elite text-to-speech technology and integrated music and SFX. Who benefits most from this upgrade? From solo TikTokers and podcast producers to agencies managing client feedback, Studio 3.0 speeds up workflows and simplifies feedback and revisions. Multilingual support is a sleeper hit, opening doors to new audiences without extra tools. We break down the pros, cons, and limitations, including free tier restrictions and browser performance. For anyone in voice-led media, Studio 3.0 might just replace half your current tool stack. Tune in for all the practical use cases, pricing tips, and the future of voice-first production.]]></description>
    <pubDate>Wed, 17 Sep 2025 18:15:03 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250917135710858+-+elevenlabs_studio_3.mp3" length="4261584" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758132903023</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250917135710858+-+elevenlabs_studio_3.jpg"/>
  </item>

  <item>
    <title>Zoom Drops Photoreal Avatars and Cross-Platform AI Notes</title>
    <description><![CDATA[Zoom just shook up video meetings at Zoomtopia with AI Companion 3.0, launching November 2025 for paid Workplace users at no extra cost. The update packs a punch: photorealistic avatars that mimic your real expressions arrive in December, offering a polished on-camera presence for when you want to skip the makeup or hit a meeting without being camera-ready. More than just looking real, these avatars could change the culture of remote work—with norms around disclosure sure to follow.

But the feature everyone’s buzzing about? AI Companion’s new cross-platform notes. Now, Zoom’s slick note-taking follows you not just in Zoom, but across Microsoft Teams and Google Meet too. That means automatic summaries, action items, and follow-ups no matter which platform you or your clients prefer, slashing busywork and context-switching for agencies, creators, educators, and podcasters alike.

Zoom 3.0 ups the fidelity too: 60fps video meetings, 1080p and 4K sharing hit Zoom Rooms for smoother design reviews, crisper content walkthroughs, and vibrant classroom sessions. “Free up my time” agentic features debut as well, with proactive nudges to keep you focused and drop low-value calls, moving Zoom’s AI from passive notetaker to active meeting coordinator.

The competition is fierce, with Microsoft, Google, and Cisco all dropping AI-powered meeting tools and avatars. But Zoom’s cross-platform notes and photoreal avatars are a bold play, aiming for both style and serious productivity gains. Expect companies to clarify policies on avatar use and watch how deep Zoom’s integration really goes—but if the rollout lives up to the hype, your next meeting might just star your digital twin and a smarter notetaker.]]></description>
    <pubDate>Fri, 19 Sep 2025 20:11:25 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250919160158448+-+zoom_ai_companion_3.mp3" length="3548136" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758312685004</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250919160158448+-+zoom_ai_companion_3.jpg"/>
  </item>

  <item>
    <title>Lucy Edit Dev: AI Video Edits with One Command</title>
    <description><![CDATA[Get ready for your fastest video edit yet. Today, Zane and Pippa break down Lucy Edit Dev from Decart AI—an open-weight, instruction-following video editor now on Hugging Face. Unlike text-to-video generators, Lucy Edit Dev lets you edit existing footage with natural language prompts: swap a kimono, replace a background, or turn a barista into an astronaut, all without painstaking masking. Powered by a VAE and Diffusion Transformer backbone in the tradition of Wan 2.2, Lucy Edit Dev keeps shot composition, motion, and timing stable while delivering precise instruction-driven edits. It is dev-focused, open for research and R and D, but under a non-commercial license, making it perfect for prototyping, benchmarking, or personal creative exploration. Find out why creators, marketers, designers, and researchers are excited about this new “do what I said” tool for efficient A B testing and visual experimentation. We cover setup needs, real-world uses, limitations like GPU compute, edge cases and license restrictions, plus how Lucy Edit Dev stacks up against Runway, Pika, Luma, and Adobe’s video AI tools. Whether you are tired of tedious rotoscoping or want to quickly generate multiple style options for your YouTube video, discover if Lucy Edit Dev unlocks a new layer of creative control and speed. Listen in for takeaways, practical tips, and the tagline memes you did not know you needed.]]></description>
    <pubDate>Tue, 23 Sep 2025 00:41:02 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250922195704275+-+lucy_edit_dev_hf.mp3" length="3556896" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758588062872</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250922195704275+-+lucy_edit_dev_hf.jpg"/>
  </item>

  <item>
    <title>Qwen3-Omni: Real-Time Multimodal AI Goes Open Source</title>
    <description><![CDATA[Can AI really respond in under a second and handle text, video, speech, and images—all at once? Meet Qwen3-Omni from Alibaba’s Qwen team. This open-source, Apache 2.0-licensed model combines multimodal understanding with real-time, streaming speech. Qwen3-Omni features a clever split: the Thinker does the smart perception, while the Talker delivers lightning-fast voice feedback, shrinking typical multi-tool workflows into one live loop. The big advantage? Sub-second response times reported as low as 211 milliseconds, open weights, and legal clarity for commercial use. Whether you’re a YouTuber wanting express captions, a podcaster making global episodes, or a developer building real-time agents, Qwen3-Omni drops speed and versatility where others gatekeep. It stands out from closed rivals like GPT-4o Realtime and Google’s Astra, and even edges out open options such as SeamlessM4T with less restrictive licensing. In today’s episode, discover how Qwen3-Omni can tighten creator workflows, sharpen media searches with OCR and Vision Q&A, and give you full control over data and deployment. We break down practical use cases—for video, podcasting, design, and even Twitch streams—and reality check the claims around speed and model size. If you’re building anything voice-interactive or content-smart, Qwen3-Omni’s all-in-one approach could change your pipeline and maybe even your budget.]]></description>
    <pubDate>Tue, 23 Sep 2025 13:55:54 GMT</pubDate>
    <enclosure url="https://coey-podcast.s3.us-east-2.amazonaws.com/20250923092902312+-+qwen3_omni_multimodal.mp3" length="3111696" type="audio/mpeg" />
    <guid isPermaLink="false">bl-1758635754698</guid>
    <itunes:image href="https://coey-podcast.s3.us-east-2.amazonaws.com/Image+20250923092902312+-+qwen3_omni_multimodal.jpg"/>
  </item>
</channel>
</rss>
